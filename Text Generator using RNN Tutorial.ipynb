{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Generator using RNN Tutorial.ipynb","provenance":[],"collapsed_sections":["8EeLky-wMmAh"],"mount_file_id":"1lsUyx2nt-P795GOMe3vvV08kzpenFXl2","authorship_tag":"ABX9TyMCY4dttWU2FIPDw4Unrf6r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"OjGwcuHygZXo"},"source":["### MLDA@EEE Deep Learning Week Special:\n","# **Text Generator using RNN**"]},{"cell_type":"markdown","metadata":{"id":"NSo4cUWJhxtZ"},"source":["This notebook is part of MLDA@EEE's series of workshops during the Deep Learning week.\n","\n","Designed to run in Google Colab.\n","\n","\n","\n","In this workshop, we assumed that you have attended the workshops in pre-deep learning week and have basic knowledge of **Python** programming, **deep learning** as well as **neural network** Basics.\n","If not, don't worry, as you will be instructed step by step in this pratical session to apply what you learnt during the tutorial session. If you encounter any technical issues or need assistance from us, you can ask us in the ZOOM chat and a helper will come to you as soon as possible.\n","\n","The structure of this pratical session is listed below:\n","1. Text Processing Basics\n","2. RNN Building Basics\n","3. Lyrics Generator\n","4. Shakespeare Generator\n","\n","\n","### **Connect to GPU instance (Recommend)**\n","To connect to GPU instance on Google Colab, follow the instruction below\n","\n","Edit > Notebook settings > Hardware accelerator > GPU"]},{"cell_type":"markdown","metadata":{"id":"-rGPDW--lKI-"},"source":["## **1. Text Processing Basics**"]},{"cell_type":"markdown","metadata":{"id":"GsXF1GX9eSO0"},"source":["Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation."]},{"cell_type":"markdown","metadata":{"id":"SPddXodNjFhA"},"source":["To give you a more intuitive perspective, we will start with a short file 'eee-overview.txt' and do some practices on text processing basics first."]},{"cell_type":"code","metadata":{"id":"SObQe2xITjBg"},"source":["# download 'eee-overview.txt' file\n","!wget https://ycrao573.github.io/rnn-workshop/eee-overview.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxcKV1G-kJGo"},"source":["overview = open('eee-overview.txt', 'r').read()\n","# length of text is the number of characters in it\n","print('Length of text: {} characters'.format(len(overview)))\n","print('First 100 characters: \\n', overview[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sESzG1tjkcvP"},"source":["# The unique characters in the file\n","vocab = sorted(set(overview))\n","print ('{} unique characters'.format(len(vocab)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uIzWzk1QygzA"},"source":["***TASK 1: Text Pre-Processing - Special Characters Cleaning***\n","\n","Please only change the code in **\\# INSERT YOUR CODE HERE** or **None**"]},{"cell_type":"code","metadata":{"id":"30d2HUIHOzRz"},"source":["stopChars = None # list of stop charaters\n","\n","# iterate over stopChars and replace them with space\n","# use string's replace(' ', ' ') method and store in 'corpus'\n","corpus = None\n","\n","print(corpus[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6MC3HUNHO09K"},"source":["corpus_words = [i for i in corpus.split() if i]\n","corpus_words[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gpevVQR3RAFl"},"source":["map(str.strip, corpus_words)\n","vocab = sorted(set(corpus_words))\n","print('Corpus length (in words):', len(corpus_words))\n","print('Unique words in corpus: {}'.format(len(vocab)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krHsSjAf1MUG"},"source":["***TASK 2: Use Index Number to Represent the Word***\n","\n","In this task, you will assign an index to each word in string 's', e.g. {1: 'eee'}\n","\n","You are encouraged to use dict and list comprehension to perform assignment and substitution.\n"]},{"cell_type":"code","metadata":{"id":"lGJmMzjkmAYj"},"source":["s = 'the school of electrical and electronic engineering ntu eee began as one of the three'\n","# using dictionary comprehension to iterate vocab, whose index should start with zero\n","word2idx = None\n","print(word2idx)\n","# using list comprehension to iterate word2idx and replace word with number (index)\n","# print them in the below\n","# INSERT YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AGG6ljRkmBHW"},"source":["Now, we will introduce tensorflow library so that we can process our text with higher quality and efficiency.\n","\n","Let's start with tokenization:"]},{"cell_type":"code","metadata":{"id":"M6CRNUq2XO9H"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GpEXs0_5eFaU"},"source":["sentences = [\n","    'I love coffee',\n","    'I do not like tea.',\n","    'We all love MLDA!'\n","]\n","tokenizer = Tokenizer(num_words = 32)\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print('word_index: ', word_index)\n","test_sen = [\n","    'I like coffee.',\n","    'You really love tea?',\n","    'We love MLDA ah!'\n","]\n","test_seq = tokenizer.texts_to_sequences(test_sen)\n","print(test_seq)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lozABdfetTTi"},"source":["We may also consider adding oov_token. Keras lets us define an Out Of Vocab token - this will replace any unknown words with a token of our choosing. This is better than just throwing away unknown words since it tells our model there was information here."]},{"cell_type":"code","metadata":{"id":"-ATZm68WaV7e"},"source":["tokenizer = Tokenizer(num_words = 32, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print('word_index: ', word_index)\n","sequences = tokenizer.texts_to_sequences(sentences)\n","test_seq = tokenizer.texts_to_sequences(test_sen)\n","print(test_seq)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mQ9JJG97tl0f"},"source":["All the neural networks require to have inputs that have the same shape and size. However, when we pre-process and use the texts as inputs for our model e.g. LSTM, not all the sentences have the same length. In other words, naturally, some of the sentences are longer or shorter. We need to have the inputs with the same size, this is where the padding is necessary."]},{"cell_type":"code","metadata":{"id":"Z-vxpLhya3gc"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(num_words = 32, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, maxlen=8)\n","print(word_index)\n","print(sequences)\n","print(padded)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8qM52HbIotlw"},"source":["## **2. RNN Building Basics**"]},{"cell_type":"markdown","metadata":{"id":"iTl0P3QTrXVF"},"source":["**Recurrent neural networks (RNN)** are a class of neural networks that is powerful for modeling sequence data such as time series or natural language. Schematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\n","\n","The **Keras RNN API** is designed with a focus on:\n","\n","\n","*   Ease of use: the built-in keras.layers.RNN, keras.layers.LSTM, keras.layers.GRU layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n","\n","*   **Ease of customization**: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic keras.layers.RNN layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code.\n","\n","For more information about building RNN in keras, please visit TensorFlow official documentation [here](https://www.tensorflow.org/guide/keras/rnn)"]},{"cell_type":"code","metadata":{"id":"40eS9Bh1Zdv2"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EeQB9ZT-15qr"},"source":["# initialize a tokenizer first\n","tokenizer = Tokenizer()\n","\n","overview = open('eee-overview.txt', 'r').read()\n","corpus = overview.lower().split(\"\\n\")\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","print(tokenizer.word_index)\n","print(total_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AxLwCeO-jSQ5"},"source":["input_sequences = []\n","for line in corpus:\n","\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n","\tfor i in range(1, len(token_list)):\n","\t\tn_gram_sequence = token_list[:i+1]\n","\t\tinput_sequences.append(n_gram_sequence)\n","\n","# pad sequences \n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# create predictors and label\n","xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n","ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PI74kHYJjTG0"},"source":["# model building\n","model = Sequential()\n","model.add(Embedding(total_words, 32, input_length=max_sequence_len-1))\n","model.add(SimpleRNN(32))\n","model.add(Dense(total_words, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","simple_history = model.fit(xs, ys, epochs=100, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tpgGNj33kHCe"},"source":["seed_text = \"eee is\"\n","next_words = 20\n","  \n","for _ in range(next_words):\n","\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n","\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","\tpredicted = model.predict_classes(token_list, verbose=0)\n","\toutput_word = \"\"\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == predicted:\n","\t\t\toutput_word = word\n","\t\t\tbreak\n","\tseed_text += \" \" + output_word\n","print(seed_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EeLky-wMmAh"},"source":["## **3. Lyrics Generator (Word Tokenization)**"]},{"cell_type":"markdown","metadata":{"id":"-EyRi_ZBwmXe"},"source":["This tutorial demonstate how to generator text based on given text using word tokenization and RNN. The text containing the song titles and the lyrics of many famous songs of Beatles (credit: [petrosDemetrakopoulos](https://github.com/petrosDemetrakopoulos/)). So, given a sequence of words from Beatles lyrics, it can predict the next words.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"STvk_xgMp0NN"},"source":["<img src = \"https://miro.medium.com/max/2560/0*SUipu9efyQeKHdlk.\" width = 70%>"]},{"cell_type":"markdown","metadata":{"id":"T50RSvBi49Xq"},"source":["***Task 3: Build the Model and Train It!***\n","\n","In this task, you will get the chance to experience the whole process of building a lyrics generator using Tensorflow Keras."]},{"cell_type":"markdown","metadata":{"id":"dWKxNvBk27sO"},"source":["Following the steps in **section two** (very important, make sure you understand them all) and create a lyrics generator for given text file."]},{"cell_type":"code","metadata":{"id":"NUzWitBkMyER"},"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, GRU, SimpleRNN, Dense, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import regularizers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7gn8iZdZ6gbo"},"source":["!wget"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K22C-bbA3MYz"},"source":["# open the 'lyrics.txt' file\n","text = None\n","# print length of text and first 250 words of the text\n","# INSERT YOUR CODE HERE\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9aQkS97TV-I"},"source":["tokenizer = Tokenizer()\n","\n","corpus = None\n","\n","# Tokenization Process\n","# INSERT YOUR CODE HERE\n","total_words = None # length of the corpus\n","\n","print(tokenizer.word_index)\n","print(total_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6IuWOmyTen5"},"source":["input_sequences = []\n","for line in corpus:\n","\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n","\tfor i in range(1, len(token_list)):\n","\t\t# Generate n-gram sequences\n","\t\t# INSERT YOUR CODE HERE\n","\n","# max of input sequences\n","max_sequence_len = None\n","# pad sequences\n","input_sequences = None\n","\n","# create predictors and label\n","predictors, labels = None\n","label = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z2JGdKlSTi67"},"source":["model = None\n","\n","# Build your own RNN model with LSTM or GRU layers\n","# INSERT YOUR CODE HERE\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bk7Kf4eh0qzc"},"source":["# train your model and store it in history\n","history = None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DIUy3tb26Fk6"},"source":["***Task 4: Our Accuracy Graph over Epochs***"]},{"cell_type":"code","metadata":{"id":"0usM9leDWYj0"},"source":["import matplotlib.pyplot as plt\n","\n","# Use pyplot to plot a graph with x as epoch number, y as accuracy\n","def plot_graphs(history, string):\n","  #INSERT YOUR CODE HERE\n","\n","plot_graphs(history, 'accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QNhs1_m06Ip0"},"source":["***Task 5: Predict Words Using Your Starting Text***"]},{"cell_type":"code","metadata":{"id":"FLac19VEYJ75"},"source":["seed_text = \"One day\"\n","next_words = 10\n","  \n","for _ in range(next_words):\n","\t# INSERT YOUR CODE HERE\n","\n","print(seed_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YDTcPCytkDSu"},"source":["## **4. Shakespeare Generator (Character Tokenization)**"]},{"cell_type":"markdown","metadata":{"id":"VM8JxBcXwYqI"},"source":["This tutorial demonstrates how to generate text using a character-based RNN. We will work with a dataset of Shakespeare's writing from Andrej Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks. Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly."]},{"cell_type":"code","metadata":{"id":"qdF6qytakFap"},"source":["import tensorflow as tf\n","import numpy as np\n","import time\n","#Download the dataset\n","path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n","                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","#Explore the data\n","text = open(path_to_file, 'r').read()\n","print(text[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"npWbbq8z7C1S"},"source":["***Task 6: Text Processing on Character Level***"]},{"cell_type":"code","metadata":{"id":"Rq35moPIntyT"},"source":["# Create the vocab of the given text\n","vocab = None\n","print ('{} unique characters'.format(len(vocab)))\n","\n","# Creating a mapping from unique characters to indices\n","char2idx = None\n","idx2char = np.array(vocab)\n","\n","# Map the character to corresponding integer (char2idx)\n","text_as_int = np.array(None)\n","for char,_ in zip(char2idx, range(5)):\n","    print(repr(char), ':', char2idx[char])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qDhZElEn2RZ"},"source":["# The maximum length sentence we want for a single input in characters\n","seq_length = 100\n","examples_per_epoch = len(text)//(seq_length+1)\n","\n","# Create training examples / targets\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","for i in char_dataset.take(5):\n","  print(idx2char[i.numpy()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gBIS-PGjn4YA"},"source":["sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n","for item in sequences.take(5):\n","  print(repr(''.join(idx2char[item.numpy()])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGsj19yun_aM"},"source":["def split_input_target(chunk):\n","  input_text = chunk[:-1]\n","  target_text = chunk[1:]\n","  return input_text, target_text\n","dataset = sequences.map(split_input_target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"asKeTJK1oEKr"},"source":["# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","\n","dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tcbsnOlBoFHp"},"source":["# Length of the vocabulary in chars\n","vocab_size = len(vocab)\n","\n","# The embedding dimension\n","embedding_dim = 256\n","\n","# Number of RNN units\n","rnn_units = 1024"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tcFbOj2oHFG"},"source":["# Helper function for building new model\n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","\n","  model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                              batch_input_shape=[batch_size, None]),\n","    tf.keras.layers.LSTM(rnn_units,\n","                        return_sequences=True,\n","                        stateful=True,\n","                        recurrent_initializer='glorot_uniform'),\n","    tf.keras.layers.Dense(vocab_size)\n","  ])\n","\n","  return model\n","\n","# return the chosen loss parameter\n","def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5b77DXXB749n"},"source":["***Task 7: Build Shakespeare Generator Model***"]},{"cell_type":"code","metadata":{"id":"rkP4z3dYoKl3"},"source":["# Complete the model infomation below\n","model = build_model(\n","    vocab_size=len(None),\n","    embedding_dim=None,\n","    rnn_units=None,\n","    batch_size=None)\n","\n","print(model.summary())\n","\n","model.compile(optimizer=None, loss=loss, metrics=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OOH937ZspOS0"},"source":["import os\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Di8Jyg4d82sD"},"source":["# Train your model here\n","history = model.fit(dataset, epochs=20, callbacks=[checkpoint_callback], verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0iFdSFjIviPk"},"source":["# Simplify the output model with batch_size = 1 for ease of prediction\n","model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1, None]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbfotJT-pi5_"},"source":["def generate_text(model, start_string):\n","  # Evaluation step (generating text using the learned model)\n","\n","  # Number of characters to generate\n","  num_generate = 1000\n","\n","  # Converting our start string to numbers (vectorizing)\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # Empty string to store our results\n","  text_generated = []\n","\n","  # Low temperatures results in more predictable text.\n","  # Higher temperatures results in more surprising text.\n","  # Experiment to find the best setting.\n","  temperature = 1.0\n","\n","  # Here batch size == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","    predictions = model(input_eval)\n","    # remove the batch dimension\n","    predictions = tf.squeeze(predictions, 0)\n","\n","    # using a categorical distribution to predict the character returned by the model\n","    predictions = predictions / temperature\n","    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","    # We pass the predicted character as the next input to the model\n","    # along with the previous hidden state\n","    input_eval = tf.expand_dims([predicted_id], 0)\n","\n","    text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kV2EmC-e9d-3"},"source":["***Task 8: Check Your Prediction Result***"]},{"cell_type":"code","metadata":{"id":"HygvXOurq2UU"},"source":["# Based on the helper function for generating text using the learned model above\n","# print the generated text with your preferred starting string\n","\n","# INSERT YOUR CODE HERE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eKQnHcjUKZmI"},"source":["## **(Optional) Text Generation using GPT-2 Transformer**"]},{"cell_type":"markdown","metadata":{"id":"LwskL9bod5hB"},"source":["OpenAI GPT-2 model was proposed in Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever. It’s a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n","\n","GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n","\n","Here, we will use a lighter version, [GPT-2 Medium Model](https://huggingface.co/gpt2-medium) for your convenience and demo purpose."]},{"cell_type":"code","metadata":{"id":"9CRWFJGmKlnP"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIxzduTbU3DF"},"source":["from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, GPT2Config"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTK-DbTnU0PH"},"source":["model_name = \"gpt2-medium\"\n","config = GPT2Config.from_pretrained(model_name)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","gptmodel = TFGPT2LMHeadModel.from_pretrained(model_name, config=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Lt17WZYZ8-o"},"source":["from transformers import set_seed\n","set_seed(23)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ZA1oVCJVria"},"source":["input_ids = tokenizer.encode('I love machine learning and data analytics,', return_tensors='tf')\n","input_ids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdyttlcvWW9p"},"source":["output = gptmodel.generate(input_ids, max_length=15)\n","print('Output:\\n')\n","print(tokenizer.decode(output[0], skip_special_tokens=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ua1NXKQiWpEp"},"source":["sample_outputs = gptmodel.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=30,\n","    top_k=50,\n","    top_p=0.95,\n","    num_return_sequences=8\n",")\n","\n","for i, sample_output in enumerate(sample_outputs):\n","  \n","  print(\"{}: {}\".format(i, tokenizer.decode(sample_output)))"],"execution_count":null,"outputs":[]}]}