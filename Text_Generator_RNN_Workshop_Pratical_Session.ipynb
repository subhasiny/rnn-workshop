{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Generator_RNN_Workshop_Pratical_Session.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-rGPDW--lKI-",
        "8qM52HbIotlw",
        "8EeLky-wMmAh",
        "YDTcPCytkDSu",
        "eKQnHcjUKZmI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc0d72800356409b8e2de18f81c8d16e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_218fa83966df4b18af04733c59988a64",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3656f295d235475f8b1e015118008d16",
              "IPY_MODEL_199308992ba64bf2a3f79f1b91e89c84"
            ]
          }
        },
        "218fa83966df4b18af04733c59988a64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3656f295d235475f8b1e015118008d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a0e0f75f7ae64d04b54a30b912400917",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 718,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 718,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c58665b6f2c4ad5b62235ad67730e63"
          }
        },
        "199308992ba64bf2a3f79f1b91e89c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9dc34912ae724498a932ab60cc11bf69",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 718/718 [00:00&lt;00:00, 5.41kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_373e0fa13978436abf42f9973f1483f8"
          }
        },
        "a0e0f75f7ae64d04b54a30b912400917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c58665b6f2c4ad5b62235ad67730e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9dc34912ae724498a932ab60cc11bf69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "373e0fa13978436abf42f9973f1483f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ecc9b47987a46288db8552eb84e7397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c359df3958e842eba36476dfd8812476",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cdd9601157f243f296ebb6d6ed83a52d",
              "IPY_MODEL_ed3d8b2af7ff47139adb58fe152d7db0"
            ]
          }
        },
        "c359df3958e842eba36476dfd8812476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdd9601157f243f296ebb6d6ed83a52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5b8bf364255f4687b0a74dd588159cb8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f26a7f48ac04790a3c0ad9b457d3528"
          }
        },
        "ed3d8b2af7ff47139adb58fe152d7db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4f103cd0b693498385e45c6514438a3d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.94MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a00c59a908554ce6b036e35fef2e4abb"
          }
        },
        "5b8bf364255f4687b0a74dd588159cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f26a7f48ac04790a3c0ad9b457d3528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4f103cd0b693498385e45c6514438a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a00c59a908554ce6b036e35fef2e4abb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cf1a8cb79c34946a7483064dbd87247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_30353a0e69964043ac8c731a75e93d59",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f87b20c79eef4949a2a85cbbc0266d68",
              "IPY_MODEL_04e7d414d18b4d3480fcd15eb6728cc9"
            ]
          }
        },
        "30353a0e69964043ac8c731a75e93d59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f87b20c79eef4949a2a85cbbc0266d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_41bb693986004c9ea868e99b44b58038",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c64730a85a294c829d923a8b7162d015"
          }
        },
        "04e7d414d18b4d3480fcd15eb6728cc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a65cc12c0044491b7524b69d7ca733c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 2.75MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a4831c328974ea092ae95a6fa5ab0a7"
          }
        },
        "41bb693986004c9ea868e99b44b58038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c64730a85a294c829d923a8b7162d015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a65cc12c0044491b7524b69d7ca733c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a4831c328974ea092ae95a6fa5ab0a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5d1fe83938f4401a8772760274c763a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_76f6ee379f2b43c39ee05b8164429928",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1a1cdc2ec173474da19fe8b29486d150",
              "IPY_MODEL_b24abb978bb2433fae220fd9fea75982"
            ]
          }
        },
        "76f6ee379f2b43c39ee05b8164429928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1a1cdc2ec173474da19fe8b29486d150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6d248996a5ed4241be55ba98f021c579",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1419628976,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1419628976,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6efd28e3afd84a7d82cf5e34f53d326f"
          }
        },
        "b24abb978bb2433fae220fd9fea75982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_956dcee020d149cfb86dbedba50ceb02",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.42G/1.42G [00:34&lt;00:00, 41.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_81f3cb7cb8c0470eb681a9c1f4304173"
          }
        },
        "6d248996a5ed4241be55ba98f021c579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6efd28e3afd84a7d82cf5e34f53d326f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "956dcee020d149cfb86dbedba50ceb02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "81f3cb7cb8c0470eb681a9c1f4304173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjGwcuHygZXo"
      },
      "source": [
        "### MLDA@EEE Deep Learning Week Special:\n",
        "# **Text Generator using RNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSo4cUWJhxtZ"
      },
      "source": [
        "This notebook is part of MLDA@EEE's series of workshops during the Deep Learning week.\n",
        "\n",
        "Designed to run in Google Colab.\n",
        "\n",
        "\n",
        "\n",
        "In this workshop, we assumed that you have attended the workshops in pre-deep learning week and have basic knowledge of **Python** programming, **deep learning** as well as **neural network** Basics.\n",
        "If not, don't worry, as you will be instructed step by step in this pratical session to apply what you learnt during the tutorial session. If you encounter any technical issues or need assistance from us, you can ask us in the ZOOM chat and a helper will come to you as soon as possible.\n",
        "\n",
        "The structure of this pratical session is listed below:\n",
        "1. Text Processing Basics\n",
        "2. RNN Building Basics\n",
        "3. Lyrics Generator\n",
        "4. Shakespeare Generator\n",
        "\n",
        "\n",
        "### **Connect to GPU instance (Recommend)**\n",
        "To connect to GPU instance on Google Colab, follow the instruction below\n",
        "\n",
        "Edit > Notebook settings > Hardware accelerator > GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rGPDW--lKI-"
      },
      "source": [
        "## **1. Text Processing Basics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsXF1GX9eSO0"
      },
      "source": [
        "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPddXodNjFhA"
      },
      "source": [
        "To give you a more intuitive perspective, we will start with a short file 'eee-overview.txt' and do some practices on text processing basics first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SObQe2xITjBg",
        "outputId": "b3746b20-96ed-4965-a025-eb0097ea4fc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# download 'eee-overview.txt' file\n",
        "!wget https://ycrao573.github.io/rnn-workshop/eee-overview.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-29 10:33:44--  https://ycrao573.github.io/rnn-workshop/eee-overview.txt\n",
            "Resolving ycrao573.github.io (ycrao573.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to ycrao573.github.io (ycrao573.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3336 (3.3K) [text/plain]\n",
            "Saving to: â€˜eee-overview.txtâ€™\n",
            "\n",
            "\reee-overview.txt      0%[                    ]       0  --.-KB/s               \reee-overview.txt    100%[===================>]   3.26K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-09-29 10:33:44 (43.1 MB/s) - â€˜eee-overview.txtâ€™ saved [3336/3336]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxcKV1G-kJGo",
        "outputId": "f6887ce3-f618-4d7b-9cd7-fcee6c7642b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "overview = open('eee-overview.txt', 'r').read()\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(overview)))\n",
        "print('First 100 characters: \\n', overview[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 3322 characters\n",
            "First 100 characters: \n",
            " The School of Electrical and Electronic Engineering (NTU EEE) began as one of the three founding sch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sESzG1tjkcvP",
        "outputId": "4f236f16-efee-43a2-d667-9f9e254cb29a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(overview))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30d2HUIHOzRz",
        "outputId": "eee8c556-e3d4-4e4a-9735-753507e1e9dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "stopChars = [',','(',')','.','-','[',']','\"','/','&','!','\\'','?']\n",
        "corpus = overview.replace('\\n', ' ').replace('\\t',' ').lower()\n",
        "for char in stopChars:\n",
        "  corpus = corpus.replace(char, '')\n",
        "corpus = corpus.replace('  ', ' ')\n",
        "print(corpus[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the school of electrical and electronic engineering ntu eee began as one of the three founding schoo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MC3HUNHO09K",
        "outputId": "20ff5951-1c41-4f55-d9b3-e7c36f068624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "corpus_words = [i for i in corpus.split() if i]\n",
        "corpus_words[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'school', 'of', 'electrical', 'and']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpevVQR3RAFl",
        "outputId": "61fab42d-a114-439e-9892-1d874f32a228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "map(str.strip, corpus_words)\n",
        "vocab = sorted(set(corpus_words))\n",
        "print('Corpus length (in words):', len(corpus_words))\n",
        "print('Unique words in corpus: {}'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length (in words): 482\n",
            "Unique words in corpus: 257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGJmMzjkmAYj",
        "outputId": "5873cb48-dc84-4a85-a640-947f2aaf1343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "s = 'the school of electrical and electronic engineering ntu eee began as one of the three'\n",
        "word2idx = {u: i for i, u in enumerate(vocab)}\n",
        "print(word2idx)\n",
        "[word2idx[i] for i in s.split()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'$90': 0, '1000': 1, '11th': 2, '13': 3, '150': 4, '194': 5, '1985': 6, '1999': 7, '20': 8, '20000': 9, '2011': 10, '2017': 11, '2020': 12, '21': 13, '300': 14, '3000': 15, '4': 16, '50': 17, '6th': 18, '7th': 19, '9': 20, 'a': 21, 'about': 22, 'academic': 23, 'active': 24, 'activities': 25, 'administrative': 26, 'advanced': 27, 'ahead': 28, 'all': 29, 'also': 30, 'alumni': 31, 'ambitions': 32, 'an': 33, 'analytics': 34, 'and': 35, 'annualised': 36, 'aoba': 37, 'apart': 38, 'are': 39, 'areas': 40, 'around': 41, 'artificial': 42, 'as': 43, 'asian': 44, 'autonomous': 45, 'average': 46, 'backed': 47, 'backgrounds': 48, 'batch': 49, 'become': 50, 'began': 51, 'being': 52, 'besides': 53, 'big': 54, 'biomedical': 55, 'both': 56, 'broad': 57, 'by': 58, 'centre': 59, 'centres': 60, 'challenges': 61, 'close': 62, 'closely': 63, 'collaborations': 64, 'communications': 65, 'companies': 66, 'competent': 67, 'consistently': 68, 'consists': 69, 'continues': 70, 'corporate': 71, 'counting': 72, 'countries': 73, 'courses': 74, 'data': 75, 'date': 76, 'delta': 77, 'demands': 78, 'development': 79, 'diverse': 80, 'drive': 81, 'each': 82, 'eee': 83, 'eeeâ€™s': 84, 'electrical': 85, 'electronic': 86, 'electronics': 87, 'energy': 88, 'engaging': 89, 'engineering': 90, 'engineers': 91, 'ensure': 92, 'equipment': 93, 'established': 94, 'expertise': 95, 'extensive': 96, 'facilities': 97, 'faculty': 98, 'first': 99, 'five': 100, 'for': 101, 'forward': 102, 'founding': 103, 'from': 104, 'frontiers': 105, 'funding': 106, 'global': 107, 'graduate': 108, 'graduated': 109, 'graduates': 110, 'great': 111, 'group': 112, 'has': 113, 'have': 114, 'healthcare': 115, 'hosts': 116, 'hundred': 117, 'impressively': 118, 'in': 119, 'include': 120, 'including': 121, 'industry': 122, 'innovation': 123, 'innovations': 124, 'institute': 125, 'institutes': 126, 'intake': 127, 'intelligence': 128, 'intelligent': 129, 'international': 130, 'internet': 131, 'is': 132, 'january': 133, 'joint': 134, 'known': 135, 'laboratories': 136, 'largest': 137, 'launch': 138, 'launched': 139, 'local': 140, 'locallymade': 141, 'marking': 142, 'members': 143, 'million': 144, 'more': 145, 'multinational': 146, 'names': 147, 'nanyang': 148, 'now': 149, 'ntu': 150, 'nurture': 151, 'nxp': 152, 'of': 153, 'offering': 154, 'on': 155, 'one': 156, 'only': 157, 'our': 158, 'output': 159, 'over': 160, 'overseas': 161, 'partners': 162, 'photonics': 163, 'power': 164, 'produces': 165, 'professional': 166, 'programme': 167, 'programmes': 168, 'proudly': 169, 'pushing': 170, 'qs': 171, 'range': 172, 'ranked': 173, 'ranking': 174, 'rankings': 175, 'ready': 176, 'receives': 177, 'renowned': 178, 'research': 179, 'researchers': 180, 'rollsroyce': 181, 'satellite': 182, 'satellites': 183, 'schaeffler': 184, 'school': 185, 'schools': 186, 'schoolâ€™s': 187, 'set': 188, 'shanghairankings': 189, 'since': 190, 'singapore': 191, 'singapores': 192, 'singaporeâ€™s': 193, 'smrt': 194, 'sophisticated': 195, 'space': 196, 'specialisation': 197, 'st': 198, 'staff': 199, 'startups': 200, 'stateoftheart': 201, 'station': 202, 'staying': 203, 'strength': 204, 'strong': 205, 'students': 206, 'subject': 207, 'subjects': 208, 'successfully': 209, 'support': 210, 'supported': 211, 'supporting': 212, 'systems': 213, 'take': 214, 'teaching': 215, 'technical': 216, 'technological': 217, 'thales': 218, 'than': 219, 'that': 220, 'the': 221, 'then': 222, 'there': 223, 'these': 224, 'they': 225, 'things': 226, 'thousand': 227, 'three': 228, 'to': 229, 'today': 230, 'tomorrowâ€™s': 231, 'top': 232, 'topranked': 233, 'total': 234, 'transportation': 235, 'undergraduate': 236, 'universities': 237, 'university': 238, 'up': 239, 'v2x': 240, 'veloxiii': 241, 'very': 242, 'we': 243, 'wellequipped': 244, 'which': 245, 'who': 246, 'whom': 247, 'with': 248, 'witnessed': 249, 'works': 250, 'world': 251, 'worldwide': 252, 'worldâ€™s': 253, 'xsat': 254, 'year': 255, 'â€“': 256}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[221, 185, 153, 85, 35, 86, 90, 150, 83, 51, 43, 156, 153, 221, 228]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGG6ljRkmBHW"
      },
      "source": [
        "Now, we will introduce tensorflow library so that we can process our text with higher quality and efficiency.\n",
        "\n",
        "Let's start with tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6CRNUq2XO9H"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpEXs0_5eFaU",
        "outputId": "387a2c43-b059-4e32-a561-2e7dd5a0fa90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sentences = [\n",
        "    'I love coffee',\n",
        "    'I do not like tea.',\n",
        "    'We all love MLDA!'\n",
        "]\n",
        "tokenizer = Tokenizer(num_words = 32)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print('word_index: ', word_index)\n",
        "test_sen = [\n",
        "    'I like coffee...',\n",
        "    'You really love tea',\n",
        "    'We love MLDA!'\n",
        "]\n",
        "test_seq = tokenizer.texts_to_sequences(test_sen)\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_index:  {'i': 1, 'love': 2, 'coffee': 3, 'do': 4, 'not': 5, 'like': 6, 'tea': 7, 'we': 8, 'all': 9, 'mlda': 10}\n",
            "[[1, 6, 3], [2, 7], [8, 2, 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lozABdfetTTi"
      },
      "source": [
        "We may also consider adding oov_token. Keras lets us define an Out Of Vocab token - this will replace any unknown words with a token of our choosing. This is better than just throwing away unknown words since it tells our model there was information here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ATZm68WaV7e",
        "outputId": "6b7e2604-fafc-4c6d-dc02-383d7617029c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 32, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print('word_index: ', word_index)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "test_sen = [\n",
        "    'I like coffee.',\n",
        "    'You really love tea',\n",
        "    'We all love MLDA!'\n",
        "]\n",
        "test_seq = tokenizer.texts_to_sequences(test_sen)\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_index:  {'<OOV>': 1, 'i': 2, 'love': 3, 'coffee': 4, 'do': 5, 'not': 6, 'like': 7, 'tea': 8, 'we': 9, 'all': 10, 'mlda': 11}\n",
            "[[2, 7, 4], [1, 1, 3, 8], [9, 10, 3, 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ9JJG97tl0f"
      },
      "source": [
        "All the neural networks require to have inputs that have the same shape and size. However, when we pre-process and use the texts as inputs for our model e.g. LSTM, not all the sentences have the same length. In other words, naturally, some of the sentences are longer or shorter. We need to have the inputs with the same size, this is where the padding is necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-vxpLhya3gc",
        "outputId": "87c4d1ef-fd99-46a3-ee42-820573601afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 32, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, maxlen=8)\n",
        "print(word_index)\n",
        "print(sequences)\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'i': 2, 'love': 3, 'coffee': 4, 'do': 5, 'not': 6, 'like': 7, 'tea': 8, 'we': 9, 'all': 10, 'mlda': 11}\n",
            "[[2, 3, 4], [2, 5, 6, 7, 8], [9, 10, 3, 11]]\n",
            "[[ 0  0  0  0  0  2  3  4]\n",
            " [ 0  0  0  2  5  6  7  8]\n",
            " [ 0  0  0  0  9 10  3 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qM52HbIotlw"
      },
      "source": [
        "## **2. RNN Building Basics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTl0P3QTrXVF"
      },
      "source": [
        "**Recurrent neural networks (RNN)** are a class of neural networks that is powerful for modeling sequence data such as time series or natural language. Schematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\n",
        "\n",
        "The **Keras RNN API** is designed with a focus on:\n",
        "\n",
        "\n",
        "*   Ease of use: the built-in keras.layers.RNN, keras.layers.LSTM, keras.layers.GRU layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n",
        "\n",
        "*   **Ease of customization**: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic keras.layers.RNN layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code.\n",
        "\n",
        "For more information about building RNN in keras, please visit TensorFlow official documentation [here](https://www.tensorflow.org/guide/keras/rnn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40eS9Bh1Zdv2"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "corpus = open('eee-overview.txt', 'r').read()\n",
        "corpus = overview.lower().split(\"\\n\")\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxLwCeO-jSQ5"
      },
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI74kHYJjTG0"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 32, input_length=max_sequence_len-1))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "simple_history = model.fit(xs, ys, epochs=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpgGNj33kHCe"
      },
      "source": [
        "seed_text = \"eee is\"\n",
        "next_words = 20\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EeLky-wMmAh"
      },
      "source": [
        "## **3. Lyrics Generator (Word Tokenization)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EyRi_ZBwmXe"
      },
      "source": [
        "This tutorial demonstate how to generator text based on given text using word tokenization and RNN. The text containing the song titles and the lyrics of many famous songs of Beatles (credit: [petrosDemetrakopoulos](https://github.com/petrosDemetrakopoulos/)). So, given a sequence of words from Beatles lyrics, it can predict the next words.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STvk_xgMp0NN"
      },
      "source": [
        "<img src = \"https://miro.medium.com/max/2560/0*SUipu9efyQeKHdlk.\" width = 70%>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUzWitBkMyER",
        "outputId": "00324314-c504-430d-87c3-4517f15ff946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "!wget https://ycrao573.github.io/rnn-workshop/lyrics.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-02 10:55:20--  https://ycrao573.github.io/rnn-workshop/lyrics.txt\n",
            "Resolving ycrao573.github.io (ycrao573.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to ycrao573.github.io (ycrao573.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 245890 (240K) [text/plain]\n",
            "Saving to: â€˜lyrics.txtâ€™\n",
            "\n",
            "\rlyrics.txt            0%[                    ]       0  --.-KB/s               \rlyrics.txt          100%[===================>] 240.13K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-10-02 10:55:20 (8.06 MB/s) - â€˜lyrics.txtâ€™ saved [245890/245890]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAphMlwf81_c"
      },
      "source": [
        "text = open('lyrics.txt', 'r').read()\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9aQkS97TV-I",
        "outputId": "ca955cff-2acd-4bd6-90d6-adadfe57c32f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "corpus = text.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'you': 1, 'i': 2, 'the': 3, 'me': 4, 'to': 5, 'and': 6, 'a': 7, 'love': 8, 'my': 9, 'in': 10, 'be': 11, 'that': 12, \"don't\": 13, 'it': 14, 'do': 15, 'all': 16, \"i'm\": 17, 'on': 18, 'your': 19, 'of': 20, 'is': 21, 'oh': 22, 'know': 23, 'so': 24, 'she': 25, \"it's\": 26, 'for': 27, 'when': 28, 'baby': 29, 'can': 30, 'but': 31, 'now': 32, 'her': 33, 'well': 34, 'no': 35, 'if': 36, 'got': 37, 'what': 38, 'with': 39, 'want': 40, 'just': 41, 'see': 42, 'come': 43, 'say': 44, 'like': 45, 'one': 46, \"you're\": 47, \"i'll\": 48, 'girl': 49, 'yeah': 50, 'gonna': 51, 'little': 52, 'get': 53, 'go': 54, 'down': 55, 'will': 56, 'time': 57, \"she's\": 58, 'let': 59, 'never': 60, 'tell': 61, 'was': 62, 'day': 63, 'said': 64, 'yeh': 65, 'back': 66, 'we': 67, 'they': 68, 'how': 69, 'there': 70, 'way': 71, \"can't\": 72, 'make': 73, 'man': 74, 'good': 75, 'have': 76, 'over': 77, \"i've\": 78, 'here': 79, 'are': 80, 'long': 81, 'not': 82, 'he': 83, 'up': 84, 'hey': 85, 'as': 86, \"that's\": 87, 'night': 88, 'take': 89, 'out': 90, 'please': 91, 'at': 92, 'feel': 93, 'from': 94, 'wanna': 95, 'why': 96, 'need': 97, 'right': 98, 'cry': 99, 'home': 100, 'la': 101, 'hold': 102, 'ever': 103, 'this': 104, 'more': 105, 'only': 106, 'been': 107, 'give': 108, 'world': 109, 'yes': 110, 'better': 111, 'sun': 112, 'blue': 113, 'mine': 114, 'leave': 115, 'life': 116, 'his': 117, \"won't\": 118, 'too': 119, 'really': 120, 'some': 121, 'away': 122, 'eyes': 123, 'look': 124, 'nothing': 125, 'da': 126, 'where': 127, 'mm': 128, 'by': 129, 'had': 130, 'then': 131, 'goodbye': 132, 'something': 133, 'heart': 134, 'again': 135, 'honey': 136, 'shout': 137, \"ain't\": 138, 'hello': 139, 'through': 140, 'true': 141, \"there's\": 142, 'roll': 143, 'ah': 144, 'think': 145, 'would': 146, 'told': 147, 'mind': 148, 'who': 149, 'iâ€™m': 150, \"you've\": 151, 'boy': 152, 'going': 153, 'before': 154, 'call': 155, 'sweet': 156, 'hard': 157, 'things': 158, 'kiss': 159, 'shake': 160, 'dance': 161, 'together': 162, 'always': 163, 'alone': 164, 'much': 165, 'money': 166, 'old': 167, 'find': 168, 'were': 169, 'lonely': 170, 'waiting': 171, 'bye': 172, 'gotta': 173, 'loves': 174, 'tonight': 175, 'could': 176, 'head': 177, 'inside': 178, 'hear': 179, 'free': 180, \"'cos\": 181, \"you'll\": 182, 'keep': 183, 'feeling': 184, 'ooh': 185, 'should': 186, 'about': 187, 'or': 188, 'has': 189, 'them': 190, 'around': 191, 'him': 192, 'am': 193, 'believe': 194, 'bad': 195, 'says': 196, 'care': 197, 'two': 198, 'thing': 199, 'other': 200, 'any': 201, 'oo': 202, 'people': 203, 'nobody': 204, 'may': 205, 'an': 206, 'remember': 207, \"i'd\": 208, 'woman': 209, 'anything': 210, 'change': 211, 'us': 212, 'still': 213, 'happy': 214, 'shoes': 215, 'dear': 216, 'comes': 217, 'done': 218, 'did': 219, 'off': 220, 'glad': 221, 'mean': 222, 'kind': 223, 'real': 224, \"we're\": 225, 'sing': 226, 'bring': 227, 'another': 228, 'run': 229, 'hand': 230, 'fine': 231, 'sad': 232, 'found': 233, 'hide': 234, 'these': 235, 'tight': 236, 'touch': 237, \"didn't\": 238, 'yoko': 239, 'show': 240, 'knew': 241, 'made': 242, 'stay': 243, 'rain': 244, 'our': 245, 'than': 246, 'till': 247, 'darling': 248, 'dead': 249, 'help': 250, 'trying': 251, 'mama': 252, 'goes': 253, 'alright': 254, 'loving': 255, \"d'\": 256, 'today': 257, 'easy': 258, 'bop': 259, 'suede': 260, 'fun': 261, 'though': 262, 'sure': 263, 'every': 264, 'someone': 265, 'hi': 266, 'without': 267, 'words': 268, 'very': 269, 'gone': 270, 'babe': 271, 'saw': 272, 'into': 273, 'dream': 274, \"'cause\": 275, 'play': 276, 'o': 277, 'place': 278, 'rock': 279, 'sleep': 280, 'step': 281, 'understand': 282, 'morning': 283, 'buy': 284, 'penina': 285, 'looking': 286, 'many': 287, 'wait': 288, 'does': 289, 'big': 290, 'everything': 291, 'thinking': 292, 'years': 293, 'came': 294, 'after': 295, \"ev'rything\": 296, 'wrong': 297, 'gimme': 298, 'their': 299, 'ask': 300, 'once': 301, 'because': 302, 'lay': 303, 'wish': 304, 'mother': 305, 'standing': 306, 'turn': 307, 'fool': 308, 'bird': 309, 'same': 310, 'try': 311, 'chance': 312, \"'bout\": 313, 'getting': 314, 'seems': 315, 'door': 316, \"they're\": 317, 'nine': 318, 'brown': 319, 'new': 320, 'die': 321, 'beautiful': 322, 'last': 323, 'music': 324, 'sky': 325, 'since': 326, 'soul': 327, 'wo': 328, 'must': 329, 'crying': 330, 'coming': 331, \"c'mon\": 332, 'lies': 333, 'face': 334, 'while': 335, 'start': 336, 'forever': 337, \"you'd\": 338, 'shine': 339, 'boys': 340, 'lula': 341, 'making': 342, 'tears': 343, 'bill': 344, 'miss': 345, 'twist': 346, 'blues': 347, 'lover': 348, 'forget': 349, 'bit': 350, 'town': 351, 'song': 352, 'hope': 353, 'nowhere': 354, \"ev'ry\": 355, 'mi': 356, 'times': 357, 'cried': 358, 'maybe': 359, 'even': 360, 'each': 361, 'send': 362, 'knows': 363, 'cold': 364, 'hoping': 365, 'listen': 366, 'near': 367, 'such': 368, 'doing': 369, 'car': 370, 'round': 371, 'heard': 372, 'stop': 373, 'yourself': 374, 'rattle': 375, 'name': 376, 'loved': 377, 'lie': 378, 'guy': 379, 'talk': 380, 'child': 381, 'hands': 382, 'bonnie': 383, 'ob': 384, 'writer': 385, 'late': 386, 'somebody': 387, 'black': 388, 'live': 389, 'rich': 390, 'leaves': 391, \"he's\": 392, 'friends': 393, 'pretty': 394, 'sit': 395, 'doctor': 396, 'mad': 397, 'worry': 398, 'lot': 399, \"that'll\": 400, 'across': 401, 'light': 402, 'friend': 403, 'gave': 404, 'sunshine': 405, 'end': 406, 'tomorrow': 407, 'lost': 408, 'walk': 409, 'carry': 410, 'fly': 411, 'mr': 412, 'days': 413, 'dreams': 414, 'arms': 415, 'birthday': 416, 'clarabella': 417, 'sunday': 418, 'gets': 419, 'hurt': 420, 'robert': 421, 'window': 422, 'paperback': 423, 'bungalow': 424, 'smile': 425, 'eye': 426, 'sea': 427, 'enough': 428, 'left': 429, 'makes': 430, 'band': 431, 'took': 432, 'side': 433, 'chains': 434, 'week': 435, 'christmas': 436, 'bloody': 437, 'julia': 438, 'laugh': 439, 'seen': 440, 'went': 441, \"we'll\": 442, 'street': 443, 'far': 444, \"doesn't\": 445, 'thought': 446, 'those': 447, 'break': 448, 'moonlight': 449, 'pain': 450, 'fast': 451, 'dog': 452, 'leaving': 453, 'driving': 454, 'ye': 455, 'gun': 456, 'los': 457, 'paranoias': 458, 'sheila': 459, 'changed': 460, 'until': 461, 'open': 462, 'broken': 463, 'put': 464, 'might': 465, 'bom': 466, 'truth': 467, 'sitting': 468, 'feet': 469, 'afraid': 470, 'use': 471, 'move': 472, 'treat': 473, 'middle': 474, 'falling': 475, \"lovin'\": 476, 'lose': 477, 'lovely': 478, 'peace': 479, 'searching': 480, 'which': 481, 'telling': 482, 'behind': 483, 'trees': 484, 'singing': 485, 'thank': 486, 'prudence': 487, 'first': 488, 'belong': 489, \"she'll\": 490, 'rhythm': 491, 'number': 492, 'wants': 493, 'queenie': 494, 'taxman': 495, 'war': 496, 'bed': 497, 'iâ€™ll': 498, \"nothing's\": 499, 'wind': 500, 'eight': 501, 'learn': 502, 'kill': 503, 'bag': 504, 'doll': 505, 'meet': 506, 'reason': 507, 'jump': 508, 'drive': 509, 'children': 510, 'garden': 511, 'tried': 512, 'lips': 513, 'satisfied': 514, 'looked': 515, 'wonder': 516, 'silly': 517, 'tu': 518, 'strawberry': 519, 'fields': 520, \"ev'rybody\": 521, 'sleeping': 522, 'soldier': 523, 'trouble': 524, 'shaking': 525, 'bang': 526, 'rocky': 527, 'teddy': 528, 'dee': 529, 'saints': 530, 'marching': 531, 'takes': 532, 'joy': 533, 'star': 534, 'tree': 535, 'save': 536, 'sound': 537, 'shot': 538, 'quite': 539, 'sorry': 540, 'hair': 541, \"'round\": 542, 'dark': 543, 'ocean': 544, 'high': 545, 'holding': 546, 'instead': 547, 'doggone': 548, 'prision': 549, 'work': 550, 'hate': 551, 'deep': 552, 'catch': 553, 'ho': 554, 'misery': 555, 'used': 556, \"baby's\": 557, 'apart': 558, 'matter': 559, 'minute': 560, 'saying': 561, 'beethoven': 562, 'tired': 563, \"let's\": 564, 'john': 565, 'needed': 566, 'helter': 567, 'skelter': 568, 'silver': 569, 'mystery': 570, 'tour': 571, 'travelling': 572, 'anyway': 573, 'sexy': 574, 'sadie': 575, 'rather': 576, 'shines': 577, 'act': 578, 'part': 579, 'twice': 580, 'red': 581, \"isn't\": 582, 'girls': 583, 'everywhere': 584, 'looks': 585, 'ground': 586, 'beat': 587, 'close': 588, 'turns': 589, 'ring': 590, 'party': 591, 'devil': 592, 'steal': 593, 'begin': 594, 'king': 595, 'top': 596, 'taking': 597, 'hole': 598, 'dig': 599, 'someday': 600, 'letter': 601, 'whoa': 602, 'anymore': 603, 'myself': 604, 'means': 605, 'brother': 606, 'living': 607, 'anybody': 608, 'xmas': 609, 'answer': 610, 'bulldog': 611, 'jude': 612, 'pie': 613, 'sgt': 614, 'evÂ´ry': 615, 'nyc': 616, \"octopus's\": 617, 'lucky': 618, 'english': 619, 'upon': 620, 'b': 621, 'moving': 622, 'soon': 623, 'attica': 624, 'ussr': 625, 'warm': 626, \"he'll\": 627, 'besame': 628, 'moment': 629, 'past': 630, 'kitchen': 631, 'drink': 632, 'along': 633, 'uh': 634, 'needs': 635, 'heavy': 636, 'walrus': 637, 'early': 638, 'hurry': 639, 'sigh': 640, 'supposed': 641, 'road': 642, \"ev'ryone\": 643, 'set': 644, 'sha': 645, 'running': 646, 'mister': 647, 'keeps': 648, \"rockin'\": 649, 'working': 650, 'shining': 651, 'beneath': 652, 'everyone': 653, 'loser': 654, 'city': 655, 'clothes': 656, 'sometimes': 657, 'harm': 658, 'year': 659, 'goo': 660, 'softer': 661, 'agree': 662, 'enjoy': 663, 'lucille': 664, 'guilty': 665, \"mummy's\": 666, 'que': 667, 'di': 668, 'penny': 669, 'lane': 670, 'september': 671, 'news': 672, 'four': 673, 'pass': 674, 'sorrow': 675, 'hit': 676, 'confidentially': 677, 'nice': 678, 'ten': 679, 'e': 680, 'bompa': 681, 'wear': 682, 'sister': 683, 'met': 684, 'lives': 685, 'phone': 686, \"they'll\": 687, 'talking': 688, 'being': 689, 'k': 690, 'cha': 691, 'blackbird': 692, 'born': 693, 'promise': 694, 'funny': 695, 'tripper': 696, 'half': 697, 'low': 698, 'seem': 699, 'fall': 700, 'room': 701, 'shuop': 702, 'happiness': 703, 'whisper': 704, 'whenever': 705, 'hill': 706, 'sent': 707, 'lizzie': 708, 'young': 709, 'sand': 710, 'best': 711, 'son': 712, \"here's\": 713, 'wife': 714, \"we'd\": 715, 'cause': 716, 'waited': 717, 'lied': 718, 'hearts': 719, 'rita': 720, 'hammer': 721, 'pasa': 722, 'ny': 723, 'molly': 724, 'bra': 725, 'lord': 726, 'louder': 727, 'irish': 728, \"he'd\": 729, 'fell': 730, 'flat': 731, 'birds': 732, 'calls': 733, 'walking': 734, 'three': 735, 'known': 736, 'angela': 737, 'state': 738, 'stand': 739, 'knock': 740, 'buys': 741, 'queen': 742, 'gives': 743, 'begins': 744, 'mucho': 745, \"'em\": 746, 'boots': 747, 'weight': 748, 'pretend': 749, 'bother': 750, 'dizzy': 751, 'belonged': 752, 'cool': 753, 'admit': 754, 'story': 755, 'shows': 756, 'glass': 757, 'wake': 758, 'line': 759, 'nearly': 760, 'appear': 761, \"day's\": 762, 'bright': 763, 'else': 764, 'clear': 765, 'ma': 766, \"thru'\": 767, 'hung': 768, 'losing': 769, 'spite': 770, 'isolation': 771, 'na': 772, 'lend': 773, 'kitten': 774, 'dirty': 775, 'taken': 776, 'magical': 777, 'martha': 778, \"maxwell's\": 779, 'michelle': 780, 'bay': 781, 'desmond': 782, \"pepper's\": 783, 'georgia': 784, 'comb': 785, 'fill': 786, 'million': 787, 'earth': 788, 'movies': 789, 'win': 790, 'knee': 791, 'tea': 792, 'white': 793, 'yellow': 794, 'everybody': 795, 'prisioners': 796, 'word': 797, 'shoulder': 798, 'poor': 799, 'grow': 800, 'fear': 801, 'clouds': 802, 'school': 803, 'write': 804, 'plays': 805, 'plans': 806, 'hell': 807, 'pots': 808, 'pans': 809, 'itâ€™s': 810, 'crazy': 811, 'suit': 812, 'flowers': 813, 'hay': 814, 'lying': 815, 'next': 816, \"we've\": 817, 'locked': 818, 'ya': 819, 'closer': 820, 'ear': 821, 'wine': 822, 'own': 823, 'everyday': 824, 'wonderful': 825, 'postman': 826, 'tear': 827, 'feels': 828, 'wearing': 829, 'kisses': 830, 'beep': 831, 'died': 832, 'wandering': 833, 'sign': 834, 'luck': 835, 'lady': 836, \"ev'rywhere\": 837, 'tells': 838, 'rings': 839, 'sees': 840, 'knees': 841, 'kansas': 842, 'its': 843, 'tall': 844, 'superior': 845, 'merry': 846, 'ears': 847, 'under': 848, 'air': 849, 'forward': 850, 'stream': 851, 'less': 852, 'worried': 853, 'wisdom': 854, 'dreamers': 855, 'meanwhile': 856, 'meter': 857, 'mailman': 858, 'trÃ©s': 859, 'bien': 860, 'ensemble': 861, 'shade': 862, 'broke': 863, 'piggies': 864, 'sails': 865, 'sunset': 866, 'club': 867, 'sixteen': 868, 'ted': 869, 'mummy': 870, \"boy's\": 871, 'crusify': 872, 'read': 873, 'house': 874, 'having': 875, 'count': 876, 'guru': 877, 'de': 878, 'naturally': 879, 'bended': 880, 'six': 881, 'c': 882, 's': 883, 'human': 884, 'often': 885, 'rocking': 886, 'spinning': 887, 'guess': 888, 'somehow': 889, 'second': 890, 'wings': 891, 'thrill': 892, \"they've\": 893, 'medley': 894, 'called': 895, 'self': 896, 'machine': 897, 'outside': 898, 'above': 899, 'woo': 900, 'turkey': 901, 'wide': 902, 'finger': 903, 'playing': 904, 'pictures': 905, 'message': 906, 'ha': 907, 'watching': 908, 't': 909, 'blow': 910, 'held': 911, 'drag': 912, 'anna': 913, 'happened': 914, 'return': 915, 'kissing': 916, 'blame': 917, 'caught': 918, \"sittin'\": 919, 'strong': 920, 'insane': 921, 'pay': 922, 'except': 923, 'allright': 924, 'asked': 925, 'death': 926, \"everybody's\": 927, \"ev'rybody's\": 928, 'yesterday': 929, 'ago': 930, \"couldn't\": 931, 'loretta': 932, 'finally': 933, 'short': 934, 'oothss': 935, 'lead': 936, 'hare': 937, 'onion': 938, 'madonna': 939, 'golden': 940, 'sings': 941, 'suddenly': 942, \"wond'ring\": 943, 'giving': 944, 'yay': 945, 'diamond': 946, 'feelings': 947, 'dressed': 948, 'follow': 949, 'huh': 950, 'ought': 951, 'slow': 952, 'matchbox': 953, 'jealous': 954, 'hoo': 955, 'hallelujah': 956, 'corner': 957, 'ice': 958, 'fight': 959, 'skies': 960, 'fancy': 961, 'bound': 962, 'surprise': 963, 'lucy': 964, \"she'd\": 965, 'calling': 966, 'kids': 967, 'lonesome': 968, 'rest': 969, 'maid': 970, 'memphis': 971, 'tennessee': 972, 'safely': 973, 'christ': 974, 'crowd': 975, \"they'd\": 976, 'book': 977, 'cup': 978, 'thousand': 979, 'loneliness': 980, 'universe': 981, 'waves': 982, 'jai': 983, 'va': 984, 'om': 985, 'ringing': 986, 'cast': 987, 'five': 988, 'sail': 989, 'green': 990, 'meant': 991, 're': 992, 'slowly': 993, 'teacher': 994, 'key': 995, 'sweeter': 996, 'lennon': 997, 'ono': 998, 'judges': 999, 'puts': 1000, 'magazine': 1001, 'ready': 1002, 'rolling': 1003, 'shop': 1004, 'bath': 1005, \"mother's\": 1006, 'drove': 1007, 'table': 1008, 'park': 1009, 'married': 1010, 'pick': 1011, \"who's\": 1012, 'sailing': 1013, 'smiling': 1014, 'kite': 1015, 'arise': 1016, 'asleep': 1017, 'spent': 1018, 'ball': 1019, 'burn': 1020, 'jar': 1021, 'wood': 1022, 'becomes': 1023, 'ride': 1024, 'perfect': 1025, 'spell': 1026, 'yet': 1027, 'harmony': 1028, 'however': 1029, 'roam': 1030, 'fever': 1031, 'crippled': 1032, 'cute': 1033, 'painting': 1034, \"o'clock\": 1035, 'voices': 1036, 'played': 1037, 'miles': 1038, 'watch': 1039, 'pony': 1040, 'boat': 1041, 'p': 1042, 'doubt': 1043, 'seventeen': 1044, 'breaks': 1045, 'cheat': 1046, 'secret': 1047, 'taste': 1048, 'yours': 1049, \"goin'\": 1050, 'record': 1051, \"heart's\": 1052, 'spend': 1053, 'romance': 1054, 'angel': 1055, 'deceive': 1056, 'wanted': 1057, 'father': 1058, 'dirt': 1059, 'finds': 1060, 'monkey': 1061, 'fixing': 1062, 'longer': 1063, 'sight': 1064, 'soft': 1065, 'dope': 1066, 'krishna': 1067, 'cheek': 1068, 'shore': 1069, 'god': 1070, 'measure': 1071, 'full': 1072, 'songs': 1073, 'reply': 1074, 'realise': 1075, 'thinks': 1076, 'choose': 1077, \"dancin'\": 1078, \"what's\": 1079, 'beside': 1080, 'spoil': 1081, 'disappear': 1082, \"movin'\": 1083, 'ow': 1084, 'mary': 1085, \"watchin'\": 1086, 'presents': 1087, 'understands': 1088, 'trust': 1089, 'beg': 1090, 'whatever': 1091, 'u': 1092, 'evening': 1093, 'ones': 1094, 'passing': 1095, 'younger': 1096, 'bottom': 1097, 'dancer': 1098, 'speaking': 1099, 'frightened': 1100, \"haven't\": 1101, 'wa': 1102, 'lazy': 1103, 'eggman': 1104, 'eggmen': 1105, 'forgot': 1106, 'pa': 1107, 'ohoh': 1108, 'indeed': 1109, 'declare': 1110, 'whole': 1111, 'hearted': 1112, 'junelight': 1113, 'bleeding': 1114, 'float': 1115, \"talkin'\": 1116, 'wall': 1117, 'troubles': 1118, 'eat': 1119, 'winding': 1120, 'cloud': 1121, 'yard': 1122, 'heaven': 1123, 'friday': 1124, 'promising': 1125, \"sister's\": 1126, 'diamonds': 1127, 'pam': 1128, 'information': 1129, 'marie': 1130, 'belle': 1131, 'daddy': 1132, \"nature's\": 1133, 'land': 1134, 'couple': 1135, 'named': 1136, 'polythene': 1137, 'raccoon': 1138, 'sundayâ€™s': 1139, 'tax': 1140, \"teddy's\": 1141, 'youâ€™d': 1142, 'notice': 1143, 'army': 1144, 'won': 1145, 'turned': 1146, 'woke': 1147, 'holes': 1148, 'small': 1149, 'lock': 1150, 'allow': 1151, 'moon': 1152, 'okay': 1153, 'slip': 1154, 'scene': 1155, 'direction': 1156, 'h': 1157, 'ship': 1158, 'breathing': 1159, 'returning': 1160, 'share': 1161, 'few': 1162, 'thick': 1163, 'shoot': 1164, 'pulled': 1165, 'revolution': 1166, 'join': 1167, 'zoo': 1168, 'natural': 1169, 'west': 1170, 'chair': 1171, 'junior': 1172, 'behave': 1173, 'barber': 1174, 'threw': 1175, 'laid': 1176, 'both': 1177, 'fire': 1178, 'guaranteed': 1179, 'mccartney': 1180, 'dough': 1181, 'frying': 1182, 'pan': 1183, 'slap': 1184, 'slander': 1185, 'liquor': 1186, 'fruit': 1187, 'carl': 1188, 'perkins': 1189, 'mirror': 1190, 'carol': 1191, 'joint': 1192, 'loud': 1193, \"makin'\": 1194, 'stars': 1195, 'cleanup': 1196, 'water': 1197, 'counting': 1198, 'magic': 1199, 'shoe': 1200, 'church': 1201, 'judge': 1202, 'local': 1203, 'meeting': 1204, 'ticket': 1205, 'stands': 1206, 'sunny': 1207, 'hour': 1208, 'row': 1209, 'treating': 1210, 'george': 1211, 'nights': 1212, 'tasting': 1213, \"there'll\": 1214, 'missing': 1215, \"rollin'\": 1216, 'reel': 1217, 'important': 1218, 'desert': 1219, 'nay': 1220, 'stone': 1221, 'marry': 1222, 'special': 1223, 'works': 1224, 'ee': 1225, 'arrive': 1226, 'between': 1227, 'eleanor': 1228, 'rigby': 1229, 'waits': 1230, 'writing': 1231, 'rise': 1232, 'stops': 1233, 'filling': 1234, 'ran': 1235, 'kept': 1236, \"weren't\": 1237, 'lasted': 1238, 'jojo': 1239, 'joe': 1240, 'rules': 1241, 'hiding': 1242, 'changing': 1243, 'single': 1244, 'voice': 1245, 'most': 1246, 'stick': 1247, 'alan': 1248, 'everytime': 1249, 'speak': 1250, 'weak': 1251, 'flow': 1252, 'clue': 1253, \"tryin'\": 1254, 'ends': 1255, 'bible': 1256, 'jesus': 1257, 'homeward': 1258, 'lullaby': 1259, 'smiles': 1260, 'proud': 1261, 'invites': 1262, 'pride': 1263, 'telephone': 1264, 'although': 1265, 'realize': 1266, 'dresses': 1267, 'jubilee': 1268, 'jamboree': 1269, 'mood': 1270, 'knocking': 1271, 'date': 1272, 'sally': 1273, 'uncle': 1274, 'eh': 1275, \"havin'\": 1276, 'log': 1277, 'cos': 1278, 'wishing': 1279, 'dreaming': 1280, 'blind': 1281, 'brings': 1282, 'fix': 1283, 'sometime': 1284, 'appreciate': 1285, 'ways': 1286, \"majesty's\": 1287, 'knowing': 1288, 'sheep': 1289, 'measured': 1290, 'refrain': 1291, 'hollywood': 1292, 'hot': 1293, 'smart': 1294, 'pigs': 1295, 'joob': 1296, 'mess': 1297, 'anyone': 1298, \"ev'ryone's\": 1299, 'upset': 1300, 'filled': 1301, 'bigger': 1302, 'papa': 1303, 'exactly': 1304, \"dif'rent\": 1305, 'cannot': 1306, 'monday': 1307, 'phony': 1308, 'lift': 1309, 'keeping': 1310, 'contend': 1311, 'vantage': 1312, 'lovers': 1313, 'danger': 1314, 'heartache': 1315, 'shove': 1316, 'sinclair': 1317, 'fat': 1318, 'bliss': 1319, 'model': 1320, 'cover': 1321, 'station': 1322, 'maggie': 1323, 'mae': 1324, 'wrote': 1325, 'maxwell': 1326, 'oan': 1327, 'sont': 1328, 'les': 1329, 'mots': 1330, 'qui': 1331, 'vont': 1332, 'mountain': 1333, 'london': 1334, 'norwegian': 1335, 'drinking': 1336, 'lets': 1337, 'cats': 1338, 'market': 1339, 'built': 1340, \"trav'ling\": 1341, 'fooling': 1342, 'northern': 1343, 'chords': 1344, 'job': 1345, 'style': 1346, 'banker': 1347, 'suburban': 1348, 'clean': 1349, 'northwest': 1350, 'mountie': 1351, 'peter': 1352, 'pair': 1353, 'chills': 1354, 'peacefully': 1355, 'mommy': 1356, 'suprise': 1357, 'sincere': 1358, 'blew': 1359, 'film': 1360, 'downstairs': 1361, 'noticed': 1362, 'hat': 1363, 'tune': 1364, 'flying': 1365, 'endless': 1366, 'bet': 1367, 'oscar': 1368, 'plat': 1369, 'plainly': 1370, 'biggest': 1371, 'perfection': 1372, 'repeat': 1373, 'neat': 1374, 'seven': 1375, 'd': 1376, 'f': 1377, 'j': 1378, 'chop': 1379, 'skip': 1380, 'rope': 1381, 'game': 1382, 'saved': 1383, 'wonders': 1384, 'swing': 1385, 'dies': 1386, 'turning': 1387, 'future': 1388, 'coffee': 1389, 'waste': 1390, 'power': 1391, 'wives': 1392, 'trigger': 1393, 'mates': 1394, 'letâ€™s': 1395, 'movement': 1396, 'rights': 1397, 'paper': 1398, 'flight': 1399, 'hardly': 1400, 'case': 1401, 'ukraine': 1402, 'moscow': 1403, \"georgia's\": 1404, 'south': 1405, \"daddy's\": 1406, 'kid': 1407, 'dime': 1408, 'jukebox': 1409, \"junior's\": 1410, 'cat': 1411, 'intention': 1412, 'letting': 1413, 'curtain': 1414, 'queue': 1415, 'davis': 1416, 'jeans': 1417, 'store': 1418, 'cross': 1419, 'busy': 1420, 'toys': 1421, 'empty': 1422, 'pole': 1423, 'scratch': 1424, 'benefit': 1425, 'hendersons': 1426, 'fair': 1427, 'men': 1428, 'performs': 1429, 'saturday': 1430, 'production': 1431, 'horse': 1432, 'solid': 1433, 'dearest': 1434, 'jay': 1435, 'fools': 1436, 'reach': 1437, 'fate': 1438, 'among': 1439, 'aw': 1440, 'foot': 1441, 'heat': 1442, 'gods': 1443, 'heavens': 1444, 'angels': 1445, 'oracle': 1446, 'spoken': 1447, 'mothers': 1448, 'body': 1449, 'thirty': 1450, 'joo': 1451, 'roller': 1452, 'joker': 1453, 'below': 1454, 'mask': 1455, 'paint': 1456, 'tie': 1457, 'mamma': 1458, 'skin': 1459, 'cooking': 1460, 'piano': 1461, 'teaser': 1462, 'driver': 1463, 'greet': 1464, 'brand': 1465, 'fading': 1466, 'smiled': 1467, 'celebrate': 1468, 'penetrate': 1469, 'radiate': 1470, 'imitate': 1471, 'indicate': 1472, 'syndicate': 1473, 'useless': 1474, \"wouldn't\": 1475, 'trip': 1476, \"m'bop\": 1477, 'fingertips': 1478, 'aaahhh': 1479, 'bundle': 1480, 'concieve': 1481, 'awoke': 1482, 'roses': 1483, 'fragrant': 1484, 'meadows': 1485, 'dawn': 1486, 'card': 1487, 'mail': 1488, 'tchaikowsky': 1489, 'diddle': 1490, \"playin'\": 1491, \"nothin'\": 1492, 'wiggles': 1493, 'partner': 1494, 'quit': 1495, 'chances': 1496, 'hugs': 1497, 'anytime': 1498, 'helps': 1499, 'national': 1500, 'latest': 1501, 'lots': 1502, 'wow': 1503, 'footsteps': 1504, 'front': 1505, 'unfair': 1506, 'screen': 1507, 'understood': 1508, 'breaking': 1509, 'picks': 1510, 'mckenzie': 1511, 'sermon': 1512, 'lifetime': 1513, 'deeper': 1514, 'higher': 1515, \"evrybody's\": 1516, 'disagree': 1517, 'aches': 1518, 'linger': 1519, 'grass': 1520, 'taught': 1521, 'angry': 1522, 'sick': 1523, 'headed': 1524, 'haired': 1525, 'soap': 1526, 'regret': 1527, 'tommy': 1528, 'paul': 1529, 'concept': 1530, 'slumbers': 1531, 'closed': 1532, 'yi': 1533, 'almost': 1534, 'hurting': 1535, 'imagine': 1536, 'imagined': 1537, 'sin': 1538, 'ly': 1539, \"wasn't\": 1540, 'walked': 1541, 'forgive': 1542, 'crossed': 1543, 'frown': 1544, 'mistake': 1545, 'kick': 1546, 'unless': 1547, \"'way\": 1548, 'folks': 1549, 'shook': 1550, 'started': 1551, 'begging': 1552, 'pray': 1553, '1': 1554, '2': 1555, 'stepping': 1556, 'disappointment': 1557, 'wicked': 1558, 'weep': 1559, 'aunt': 1560, 'matches': 1561, \"ol'\": 1562, 'peaches': 1563, 'worth': 1564, 'loted': 1565, 'peasant': 1566, \"foolin'\": 1567, 'hates': 1568, 'realised': 1569, 'happen': 1570, 'vain': 1571, 'learns': 1572, 'hang': 1573, 'tenderly': 1574, 'dry': 1575, 'homing': 1576, 'seemed': 1577, 'spells': 1578, 'begun': 1579, 'easier': 1580, 'streams': 1581, 'hela': 1582, 'heba': 1583, 'heloa': 1584, \"anybody's\": 1585, 'assured': 1586, 'opened': 1587, 'doors': 1588, 'slide': 1589, 'wave': 1590, 'woof': 1591, 'north': 1592, 'ceiling': 1593, 'arrow': 1594, 'piercing': 1595, 'suppose': 1596, 'pepper': 1597, 'freaks': 1598, 'learned': 1599, 'facing': 1600, 'denied': 1601, 'stupid': 1602, 'tuesday': 1603, 'nau': 1604, 'ghty': 1605, 'policemen': 1606, 'showed': 1607, 'hanging': 1608, 'sided': 1609, 'weaving': 1610, 'planned': 1611, 'winds': 1612, 'rivers': 1613, 'shy': 1614, 'laughing': 1615, 'imply': 1616, 'theyâ€™ll': 1617, 'tries': 1618, 'nasty': 1619, 'habit': 1620, 'disappearing': 1621, \"diff'rence\": 1622, 'afternoon': 1623, 'sliping': 1624, 'fingers': 1625, 'yawning': 1626, 'wink': 1627, 'putting': 1628, 'joke': 1629, 'brain': 1630, 'weeks': 1631, 'sir': 1632, 'train': 1633, 'wondering': 1634, 'toes': 1635, 'carve': 1636, 'places': 1637, 'affection': 1638, 'expect': 1639, 'floating': 1640, 'cake': 1641, \"who'd\": 1642, 'nobodyâ€™d': 1643, 'simple': 1644, \"diff'rent\": 1645, 'evÂ´rything': 1646, 'felt': 1647, 'began': 1648, 'control': 1649, 'shivering': 1650, 'swallowing': 1651, 'selling': 1652, 'bastards': 1653, 'junk': 1654, 'motorcars': 1655, 'handlebars': 1656, 'bicycles': 1657, 'parachutes': 1658, 'bags': 1659, 'faint': 1660, 'manage': 1661, 'pappie': 1662, 'thanks': 1663, 'cloudy': 1664, 'prairies': 1665, 'prairie': 1666, 'wallflowers': 1667, 'wanting': 1668, 'reaching': 1669, 'asking': 1670, 'dinner': 1671, 'satisfy': 1672, 'picture': 1673, 'kaleidoscope': 1674, 'pies': 1675, 'newspaper': 1676, 'invitation': 1677, 'reservation': 1678, 'dying': 1679, 'sue': 1680, 'class': 1681, 'fifty': 1682, 'mustard': 1683, 'sleeps': 1684, 'shaves': 1685, 'saving': 1686, 'note': 1687, 'waving': 1688, 'homedrops': 1689, 'trickled': 1690, 'legs': 1691, 'police': 1692, 'elephants': 1693, 'memory': 1694, 'grag': 1695, 'village': 1696, 'untrue': 1697, 'paradise': 1698, 'flown': 1699, 'worked': 1700, 'heels': 1701, 'ease': 1702, 'keys': 1703, 'command': 1704, 'singer': 1705, 'twenty': 1706, 'jones': 1707, 'stays': 1708, 'shave': 1709, 'wears': 1710, 'comfort': 1711, 'begged': 1712, 'railman': 1713, 'location': 1714, \"list'ning\": 1715, 'steady': 1716, 'motor': 1717, 'pouring': 1718, 'strange': 1719, 'fireman': 1720, 'shirts': 1721, 'piggy': 1722, 'clutching': 1723, \"weather's\": 1724, 'starts': 1725, 'borrow': 1726, 'straight': 1727, 'reminiscing': 1728, 'sore': 1729, 'plan': 1730, 'somewhere': 1731, 'himself': 1732, 'checked': 1733, \"gideon's\": 1734, 'rival': 1735, 'grin': 1736, 'doc': 1737, 'charlie': 1738, 'chan': 1739, 'simon': 1740, 'smith': 1741, 'sergeant': 1742, 'gunn': 1743, 'ooo': 1744, 'drummon': 1745, 'goodness': 1746, 'wisper': 1747, 'raindrops': 1748, 'spring': 1749, 'layed': 1750, 'greatest': 1751, 'introduce': 1752, 'bathroom': 1753, 'sundays': 1754, 'tuesdays': 1755, 'ourselves': 1756, 'drives': 1757, 'whispers': 1758, 'loneliest': 1759, 'creatures': 1760, 'ugly': 1761, 'duck': 1762, 'surrender': 1763, 'amore': 1764, 'dare': 1765, 'ireland': 1766, 'gal': 1767, 'fellers': 1768, 'boston': 1769, 'philadelphia': 1770, 'texas': 1771, 'frisco': 1772, 'saint': 1773, 'louis': 1774, 'orleans': 1775, 'rainbows': 1776, 'discover': 1777, 'prove': 1778, 'paris': 1779, 'tiger': 1780, 'farther': 1781, 'travels': 1782, 'leads': 1783, 'pool': 1784, 'goddman': 1785, 'genocide': 1786, 'aye': 1787, 'goodbyes': 1788, 'unwise': 1789, 'grade': 1790, 'photograph': 1791, 'lights': 1792, 'stood': 1793, 'stared': 1794, 'lords': 1795, 'dragged': 1796, 'drank': 1797, 'coat': 1798, 'grabbed': 1799, 'bus': 1800, 'seconds': 1801, 'upstairs': 1802, 'smoke': 1803, 'spoke': 1804, 'blackburn': 1805, 'lancashire': 1806, 'albert': 1807, 'hall': 1808, 'rainclouds': 1809, 'papercup': 1810, 'slither': 1811, 'univer': 1812, 'se': 1813, 'pools': 1814, 'drifting': 1815, 'possessing': 1816, 'caressing': 1817, 'images': 1818, 'thoughts': 1819, 'meander': 1820, 'restless': 1821, 'letterbox': 1822, 'tumble': 1823, 'blindly': 1824, 'sounds': 1825, 'laughter': 1826, 'shades': 1827, 'views': 1828, 'inciting': 1829, 'inviting': 1830, 'limitless': 1831, 'undying': 1832, 'suns': 1833, 'beggin': 1834, 'rehearsal': 1835, 'g': 1836, 'pink': 1837, 'orange': 1838, 'sung': 1839, 'shown': 1840, 'prized': 1841, 'possessions': 1842, 'awoken': 1843, 'prison': 1844, 'millions': 1845, 'political': 1846, 'thereÂ´s': 1847, 'weÂ´re': 1848, 'hopes': 1849, 'watches': 1850, 'sisters': 1851, 'brothers': 1852, 'reaches': 1853, 'different': 1854, 'races': 1855, 'jailhouse': 1856, 'equality': 1857, 'naking': 1858, 'thin': 1859, 'unhappy': 1860, 'sympathize': 1861, 'faded': 1862, 'towers': 1863, 'forthy': 1864, 'widowed': 1865, 'media': 1866, 'blames': 1867, 'rockefeller': 1868, 'thatâ€™s': 1869, 'weâ€™re': 1870, 'justice': 1871, 'suffocation': 1872, 'wathc': 1873, 'nowâ€™s': 1874, 'hatred': 1875, 'judgement': 1876, 'mattes': 1877, 'traveled': 1878, 'tuned': 1879, 'flew': 1880, 'miami': 1881, 'beach': 1882, 'dreadful': 1883, 'gee': 1884, 'unpack': 1885, 'disconnect': 1886, 'snow': 1887, 'peaked': 1888, 'mountains': 1889, 'farm': 1890, 'balalaikas': 1891, 'comrad': 1892, 'moved': 1893, 'neighborhood': 1894, 'sits': 1895, 'tacks': 1896, 'teachers': 1897, 'gum': 1898, \"girl's\": 1899, 'worries': 1900, 'poop': 1901, 'hula': 1902, 'hoop': 1903, 'cut': 1904, 'canary': 1905, 'fed': 1906, 'neighbors': 1907, 'cocker': 1908, 'spaniel': 1909, 'laundramat': 1910, \"mama's\": 1911, 'softly': 1912, \"sighin'\": 1913, 'breeze': 1914, \"cryin'\": 1915, 'badge': 1916, 'wander': 1917, 'swans': 1918, 'mabel': 1919, 'wheel': 1920, 'cradle': 1921, 'gene': 1922, 'vincent': 1923, 'sheriff': 1924, 'tex': 1925, 'team': 1926, \"walkin'\": 1927, 'monsters': 1928, 'prayer': 1929, 'age': 1930, 'patient': 1931, 'meantime': 1932, 'happens': 1933, 'sean': 1934, 'streaming': 1935, 'ploys': 1936, 'forty': 1937, 'creating': 1938, 'multiple': 1939, 'fence': 1940, 'settle': 1941, 'blows': 1942, 'trampoline': 1943, 'pablo': 1944, 'fanques': 1945, 'horses': 1946, 'hoops': 1947, 'garters': 1948, 'lastly': 1949, 'hogshead': 1950, 'challenge': 1951, 'celebrated': 1952, 'feat': 1953, 'bishopsgate': 1954, 'flys': 1955, 'messrs': 1956, 'assure': 1957, 'public': 1958, 'none': 1959, 'course': 1960, 'henry': 1961, 'dances': 1962, 'waltz': 1963, 'tricks': 1964, 'demonstrate': 1965, 'summersets': 1966, 'undertake': 1967, 'preparation': 1968, 'splendid': 1969, 'topping': 1970, 'divine': 1971, 'whispering': 1972, 'adore': 1973, 'sunken': 1974, 'fog': 1975, 'l': 1976, 'themselves': 1977, 'policeman': 1978, 'breath': 1979, 'raised': 1980, 'flut': 1981, 'battered': 1982, 'razor': 1983, 'shattered': 1984, 'kicked': 1985, 'thrown': 1986, 'tools': 1987, 'aprision': 1988, 'rule': 1989, 'wardnes': 1990, 'seasons': 1991, 'vision': 1992, 'jumping': 1993, 'highway': 1994, 'cutie': 1995, \"ma'am\": 1996, 'jammed': 1997, 'tap': 1998, 'overcome': 1999, 'pillow': 2000, 'invitations': 2001, 'celebrations': 2002, \"coachin'\": 2003, 'dungeries': 2004, 'already': 2005, \"abc's\": 2006, 'heh': 2007, 'bread': 2008, 'enemies': 2009, 'absolutely': 2010, 'rats': 2011, 'aboard': 2012, 'travel': 2013, 'wherever': 2014, 'centre': 2015, 'circle': 2016, \"temperature's\": 2017, 'rising': 2018, 'aching': 2019, 'goose': 2020, 'pimple': 2021, 'bone': 2022, 'freeze': 2023, 'hours': 2024, 'praying': 2025, 'grooving': 2026, 'eyeball': 2027, 'holly': 2028, 'toejam': 2029, 'football': 2030, 'coca': 2031, 'cola': 2032, 'gumboot': 2033, 'sideboard': 2034, 'spinal': 2035, 'cracker': 2036, 'armchair': 2037, 'disease': 2038, 'coaster': 2039, 'warning': 2040, 'muddy': 2041, 'mojo': 2042, 'filter': 2043, 'sonny': 2044, 'race': 2045, 'collar': 2046, 'itself': 2047, 'hymn': 2048, 'color': 2049, 'marigold': 2050, 'breakfast': 2051, 'parlour': 2052, 'picking': 2053, 'playroom': 2054, 'childrens': 2055, 'holiday': 2056, 'duchess': 2057, 'kircaldy': 2058, 'arriving': 2059, 'duke': 2060, 'problems': 2061, 'bee': 2062, 'twelve': 2063, 'seance': 2064, 'specially': 2065, 'lark': 2066, 'ski': 2067, 'ies': 2068, 'chi': 2069, 'ld': 2070, 'daisy': 2071, 'chain': 2072, 'wilt': 2073, 'flower': 2074, \"spirit's\": 2075, 'v': 2076, 'track': 2077, 'goddess': 2078, 'hog': 2079, 'moondog': 2080, 'stoney': 2081, 'treasure': 2082, 'eternally': 2083, 'complain': 2084, 'pleasing': 2085, 'beyond': 2086, 'compare': 2087, 'zoom': 2088, \"cross'd\": 2089, 'danced': 2090, \"mo'\": 2091, \"searchin'\": 2092, \"darlin'\": 2093, 'imprisoned': 2094, 'touched': 2095, 'lingers': 2096, \"'though\": 2097, \"evr'rybody\": 2098, 'phon': 2099, 'plain': 2100, 'bells': 2101, 'winging': 2102, 'girlfriend': 2103, 'patiently': 2104, 'passed': 2105, 'check': 2106, 'deliver': 2107, 'sooner': 2108, 'awrite': 2109, \"jumpin'\": 2110, 'jockey': 2111, \"temp'rature's\": 2112, \"risin'\": 2113, \"blowin'\": 2114, 'fuse': 2115, \"beatin'\": 2116, \"asingin'\": 2117, 'pneumonia': 2118, 'arthritis': 2119, 'review': 2120, 'trifle': 2121, 'further': 2122, \"mornin'\": 2123, \"givin'\": 2124, \"warnin'\": 2125, 'fiddle': 2126, 'glow': 2127, 'worm': 2128, \"spinnin'\": 2129, 'oughta': 2130, 'badly': 2131, 'madly': 2132, 'wring': 2133, 'split': 2134, 'tantalize': 2135, 'thrilling': 2136, 'em': 2137, 'doris': 2138, 'matt': 2139, 'busby': 2140, \"rock'n'roll\": 2141, 'stroll': 2142, 'fore': 2143, 'bride': 2144, 'helping': 2145, 'succeed': 2146, 'health': 2147, 'lace': 2148, \"powder's\": 2149, 'tomboy': 2150, 'guys': 2151, 'lasts': 2152, 'clock': 2153, 'aticking': 2154, 'mantel': 2155, 'shelf': 2156, 'amoving': 2157, 'doubted': 2158, 'crash': 2159, 'famous': 2160, 'gril': 2161, 'prospects': 2162, 'peanuts': 2163, 'rice': 2164, 'wedding': 2165, 'darning': 2166, 'stocks': 2167, \"re's\": 2168, 'buried': 2169, 'wiping': 2170, 'walks': 2171, 'grave': 2172, 'presses': 2173, 'breast': 2174, 'craks': 2175, 'colourful': 2176, 'kindness': 2177, 'wakes': 2178, 'loner': 2179, \"n't\": 2180, 'tuscon': 2181, 'arizona': 2182, 'california': 2183, 'martin': 2184, 'heel': 2185, 'neck': 2186, 'sweater': 2187, \"teacher's\": 2188, 'cruel': 2189, 'hearing': 2190, 'uptight': 2191, 'sighted': 2192, 'narrow': 2193, 'minded': 2194, 'hypocritics': 2195, 'reading': 2196, 'neurotic': 2197, 'psychotic': 2198, 'pig': 2199, 'politicians': 2200, 'bellied': 2201, 'tricky': 2202, 'dicky': 2203, 'hubbard': 2204, 'pocketful': 2205, 'mope': 2206, 'seeing': 2207, 'lipped': 2208, 'condescending': 2209, 'mommies': 2210, 'chauvinists': 2211, 'scenes': 2212, 'schizophrenic': 2213, 'ego': 2214, 'centric': 2215, 'paranoic': 2216, 'prima': 2217, 'donnas': 2218, \"son't\": 2219, 'promises': 2220, 'acts': 2221, 'pleasu': 2222, 'earn': 2223, 'leasure': 2224, 'food': 2225, \"window's\": 2226, \"money's\": 2227, \"living's\": 2228, 'heartbeat': 2229, 'flesh': 2230, 'bagism': 2231, 'shagism': 2232, 'dragism': 2233, 'madism': 2234, 'ra': 2235, 'gism': 2236, 'tagism': 2237, 'thisism': 2238, 'thatism': 2239, 'ministers': 2240, 'sinisters': 2241, 'banisters': 2242, 'canisters': 2243, 'bishops': 2244, 'fishops': 2245, 'rabbis': 2246, 'popeyes': 2247, 'byes': 2248, 'regulations': 2249, 'integrations': 2250, 'meditations': 2251, 'united': 2252, 'nations': 2253, 'congratulations': 2254, 'timmy': 2255, 'leary': 2256, 'rosemary': 2257, 'smothers': 2258, 'bobby': 2259, 'dylan': 2260, 'cooper': 2261, 'derek': 2262, 'taylor': 2263, 'norman': 2264, 'mailer': 2265, 'ginsberg': 2266, 'goosepimples': 2267, 'mercy': 2268, 'sewed': 2269, 'electric': 2270, 'wire': 2271, 'temperature': 2272, 'ows': 2273, 'bent': 2274, 'backed': 2275, 'fulips': 2276, 'castiron': 2277, 'dovetail': 2278, 'ching': 2279, 'tarot': 2280, 'hitler': 2281, 'kennedy': 2282, 'buddha': 2283, 'mantra': 2284, 'gita': 2285, 'yoga': 2286, 'kings': 2287, 'elvis': 2288, 'zimmerman': 2289, 'beatles': 2290, 'reality': 2291, 'dreamweaver': 2292, 'reborn': 2293, 'awake': 2294, 'blab': 2295, 'burns': 2296, 'shady': 2297, \"how's\": 2298, 'heading': 2299, 'ruin': 2300, 'decide': 2301, 'skirts': 2302, 'flirt': 2303, 'gear': 2304, 'lingered': 2305, 'excite': 2306, 'oogh': 2307, \"ev'ryday\": 2308, 'bees': 2309, 'apologize': 2310, 'resign': 2311, 'diamonmd': 2312, 'talkin': 2313, 'rep': 2314, 'peep': 2315, 'thet': 2316, 'clown': 2317, 'deserve': 2318, 'whim': 2319, 'against': 2320, 'modern': 2321, 'jazz': 2322, 'darn': 2323, 'beauty': 2324, 'melody': 2325, 'symphony': 2326, \"'cross\": 2327, 'tracks': 2328, 'wail': 2329, 'sax': 2330, \"hurrican'\": 2331, 'jokey': 2332, \"drinkin'\": 2333, 'brew': 2334, 'tango': 2335, 'mambo': 2336, 'congo': 2337, 'summernight': 2338, 'beam': 2339, '3': 2340, '4': 2341, '5': 2342, '6': 2343, '7': 2344, '8': 2345, '9': 2346, 'ringo': 2347, 'teen': 2348, 'yake': 2349, 'books': 2350, 'boyfriend': 2351, 'bbbbb': 2352, 'lotta': 2353, 'cares': 2354, \"cummin'\": 2355, 'ducked': 2356, 'alley': 2357, 'puppy': 2358, 'runs': 2359, 'moan': 2360, 'apologise': 2361, 'pleas': 2362, 'ours': 2363, 'wing': 2364, 'soooo': 2365, 'favorite': 2366, 'holds': 2367, 'chick': 2368, 'misses': 2369, 'aquainted': 2370, 'velvet': 2371, 'lizard': 2372, 'pane': 2373, 'multicolored': 2374, 'mirrors': 2375, 'hob': 2376, 'nail': 2377, 'impression': 2378, 'ate': 2379, 'donated': 2380, 'bits': 2381, 'fights': 2382, 'cars': 2383, 'buses': 2384, 'planes': 2385, \"other's\": 2386, 'rough': 2387, 'twinkling': 2388, \"happen'd\": 2389, 'anubody': 2390, 'independence': 2391, 'vanish': 2392, 'haze': 2393, 'insecure': 2394, 'majesty': 2395, 'changes': 2396, 'belly': 2397, 'clod': 2398, 'winter': 2399, 'faces': 2400, 'melting': 2401, 'deny': 2402, 'believing': 2403, 'bullfrog': 2404, 'jack': 2405, 'knife': 2406, 'sweaty': 2407, 'innocence': 2408, 'wigwam': 2409, 'solitude': 2410, \"wha'd'ya\": 2411, \"d'y'know\": 2412, 'shoulders': 2413, 'colder': 2414, 'perform': 2415, 'yourskin': 2416, 'cookie': 2417, 'england': 2418, 'bigtime': 2419, 'position': 2420, 'tragic': 2421, 'became': 2422, 'legend': 2423, 'frantic': 2424, 'atlantic': 2425, 'kinds': 2426, 'kindly': 2427, 'tee': 2428, 'honeymoon': 2429, 'horizon': 2430, 'reelings': 2431, 'straights': 2432, 'jum': 2433, 'muzak': 2434, 'tough': 2435, 'ng': 2436, 'cornflake': 2437, 'van': 2438, 'corporation': 2439, 'teashirt': 2440, 'custard': 2441, 'dripping': 2442, 'dogs': 2443, 'crabalocker': 2444, 'fishwife': 2445, 'pornographic': 2446, 'priestess': 2447, 'kni': 2448, 'ckers': 2449, 'tan': 2450, 'eng': 2451, 'lish': 2452, 'expert': 2453, 'texpert': 2454, 'choking': 2455, 'smokers': 2456, 'laughs': 2457, 'hahaha': 2458, 'sty': 2459, 'snied': 2460, 'semolina': 2461, 'pilchard': 2462, 'climbing': 2463, 'eiffel': 2464, 'tower': 2465, 'elementry': 2466, 'penguin': 2467, 'kicking': 2468, 'edgar': 2469, 'allen': 2470, 'poe': 2471, \"nobody's\": 2472, 'cock': 2473, 'occupied': 2474, 'junkies': 2475, 'religion': 2476, 'cocaine': 2477, 'awww': 2478, 'grumbles': 2479, 'fusses': 2480, 'treats': 2481, 'streets': 2482, \"woman's\": 2483, 'search': 2484, 'crave': 2485, 'sinking': 2486, \"life's\": 2487, 'nervous': 2488, \"one's\": 2489, 'flowing': 2490, 'freely': 2491, 'games': 2492, 'unkind': 2493, 'confusing': 2494, 'mattered': 2495, 'endear': 2496, 'sailor': 2497, 'failure': 2498, 'lawyer': 2499, 'beggar': 2500, 'thief': 2501, 'churchman': 2502, 'hid': 2503, 'later': 2504, \"love's\": 2505, 'chip': 2506, 'stare': 2507, 'ay': 2508, 'throws': 2509, 'moar': 2510, \"an'\": 2511, 'hug': 2512, 'discovered': 2513, 'soothing': 2514, \"aren't\": 2515, 'learnt': 2516, \"stranger's\": 2517, 'wires': 2518, \"communication's\": 2519, 'valley': 2520, 'indecision': 2521, 'remind': 2522, 'stuff': 2523, 'waht': 2524, 'bandaid': 2525, 'carrey': 2526, 'score': 2527, 'scored': 2528, 'jive': 2529, 'speed': 2530, 'staring': 2531, 'slept': 2532, 'blink': 2533, 'cigarette': 2534, 'curse': 2535, 'walter': 2536, 'raleigh': 2537, 'git': 2538, 'shivers': 2539, 'wet': 2540, 'socks': 2541, 'aware': 2542, 'missed': 2543, 'wew': 2544, 'dont': 2545, 'especially': 2546, 'remain': 2547, 'moments': 2548, 'recall': 2549, 'compares': 2550, \"mem'ries\": 2551, 'meaning': 2552, 'others': 2553, 'caused': 2554, 'victim': 2555, 'piece': 2556, 'dries': 2557, 'butterflies': 2558, 'iâ€™d': 2559, 'aloud': 2560, \"i'\": 2561, 'm': 2562, 'beating': 2563, 'inscure': 2564, 'ainâ€™t': 2565, 'stir': 2566, 'wonâ€™t': 2567, 'stair': 2568, 'shooting': 2569, 'gooks': 2570, 'vietnam': 2571, 'cia': 2572, 'heâ€™d': 2573, 'theyâ€™d': 2574, 'breatthing': 2575, 'gottta': 2576, 'freee': 2577, 'jailed': 2578, 'representing': 2579, 'clutches': 2580, 'oof': 2581, 'lid': 2582, 'meaningless': 2583, 'seashell': 2584, 'shimmering': 2585, 'glimmering': 2586, 'silent': 2587, 'draw': 2588, 'darts': 2589, 'terror': 2590, 'childhood': 2591, 'bell': 2592, 'rent': 2593, 'arrives': 2594, 'suitcase': 2595, 'creep': 2596, 'nun': 2597, \"monday's\": 2598, 'bootlace': 2599, 'ending': 2600, \"wedn'sday\": 2601, 'papers': 2602, 'thursday': 2603, 'stockings': 2604, 'mending': 2605, 'mammie': 2606, 'scold': 2607, 'sugar': 2608, 'darkness': 2609, 'parted': 2610, 'lumps': 2611, 'throat': 2612, 'isle': 2613, 'sweetly': 2614, 'omit': 2615, 'rocker': 2616, \"it'll\": 2617, 'wasting': 2618, 'misplace': 2619, 'looooooooved': 2620, 'tun': 2621, 'alife': 2622, 'bought': 2623, \"who'll\": 2624, 'screw': 2625, 'sins': 2626, 'tow': 2627, 'parking': 2628, 'glimpse': 2629, 'cap': 2630, 'older': 2631, 'military': 2632, 'inquire': 2633, 'discreetly': 2634, 'paid': 2635, 'sofa': 2636, 'river': 2637, 'tangerine': 2638, 'marmelade': 2639, 'cellophane': 2640, 'towering': 2641, 'bridge': 2642, 'fountain': 2643, 'marshmallow': 2644, 'drift': 2645, 'incredibly': 2646, 'taxis': 2647, 'climb': 2648, 'plasticine': 2649, 'porters': 2650, 'ties': 2651, 'turnstile': 2652, 'lisle': 2653, 'mower': 2654, 'robbing': 2655, 'boun': 2656, 'der': 2657, \"robbin'\": 2658, 'liverpool': 2659, 'returned': 2660, 'pound': 2661, 'uo': 2662, 'satisfaction': 2663, 'aaaaah': 2664, 'conversation': 2665, 'inspiration': 2666, 'joan': 2667, 'quizzical': 2668, 'studied': 2669, 'paraphysical': 2670, 'science': 2671, 'testube': 2672, 'edison': 2673, 'majoring': 2674, 'medicine': 2675, 'jo': 2676, 'annoyed': 2677, 'avoid': 2678, 'unpleasant': 2679, 'sce': 2680, 'ene': 2681, 'max': 2682, 'creeps': 2683, 'testimonial': 2684, 'rose': 2685, 'valerie': 2686, 'screaming': 2687, 'gallery': 2688, 'noise': 2689, 'bob': 2690, 'nose': 2691, 'getter': 2692, 'shouts': 2693, 'obscene': 2694, 'distance': 2695, 'placed': 2696, 'add': 2697, 'strolling': 2698, 'hairy': 2699, 'country': 2700, 'waters': 2701, 'flies': 2702, 'field': 2703, 'swaying': 2704, 'daisies': 2705, 'explain': 2706, 'york': 2707, 'jerry': 2708, 'guitar': 2709, 'marijuana': 2710, 'peel': 2711, 'sangs': 2712, 'pope': 2713, 'smokes': 2714, 'shoved': 2715, 'singin': 2716, 'maxâ€™s': 2717, 'nitty': 2718, 'gritty': 2719, 'spread': 2720, 'plastic': 2721, 'funky': 2722, 'boogie': 2723, 'tutti': 2724, 'frutti': 2725, 'sallyâ€™s': 2726, 'preachman': 2727, 'godâ€™s': 2728, 'herring': 2729, 'staten': 2730, 'island': 2731, 'ferry': 2732, 'telly': 2733, 'fillmore': 2734, 'apollo': 2735, 'freedom': 2736, 'image': 2737, 'cycling': 2738, 'bug': 2739, 'hustle': 2740, 'ud': 2741, 'decided': 2742, 'statue': 2743, 'liberty': 2744, 'ass': 2745, 'anywhere': 2746, 'sat': 2747, 'rug': 2748, 'biding': 2749, 'talked': 2750, 'crawled': 2751, 'lit': 2752, 'vest': 2753, 'ageing': 2754, 'handing': 2755, 'writ': 2756, 'underfed': 2757, 'freak': 2758, 'sikh': 2759, 'leading': 2760, 'astray': 2761, 'mandalay': 2762, 'apple': 2763, 'cart': 2764, 'misled': 2765, \"shakin'\": 2766, \"there'd\": 2767, 'gang': 2768, 'joes': 2769, 'grab': 2770, 'squeeze': 2771, 'tease': 2772, 'spends': 2773, 'point': 2774, 'view': 2775, 'lends': 2776, 'barrow': 2777, 'trolley': 2778, \"jeweller's\": 2779, 'carat': 2780, 'dib': 2781, 'storm': 2782, 'hideaway': 2783, 'resting': 2784, 'cave': 2785, 'swim': 2786, 'coral': 2787, 'safe': 2788, 'replacing': 2789, 'thoughtless': 2790, 'escaping': 2791, 'worrying': 2792, 'imperfect': 2793, 'reject': 2794, 'faster': 2795, 'weather': 2796, 'lip': 2797, 'gald': 2798, '909': 2799, 'evÂ´rythingÂ´s': 2800, 'fare': 2801, 'correct': 2802, 'madam': 2803, 'wri': 2804, 'te': 2805, 'based': 2806, 'novel': 2807, 'lear': 2808, 'clinging': 2809, 'daily': 2810, 'pages': 2811, 'overnight': 2812, 'aldebaran': 2813, 'liquids': 2814, 'drums': 2815, 'arm': 2816, 'board': 2817, 'showing': 2818, 'photographs': 2819, 'pleasure': 2820, 'mack': 2821, 'hourglass': 2822, 'pocket': 2823, 'portrait': 2824, 'likes': 2825, 'engine': 2826, 'fish': 2827, 'summer': 2828, 'shelter': 2829, 'roundabout': 2830, 'nurse': 2831, 'poppies': 2832, 'tray': 2833, \"tho'\": 2834, 'customer': 2835, 'trim': 2836, 'rushes': 2837, 'crawling': 2838, 'worse': 2839, 'starched': 2840, 'stirring': 2841, 'styes': 2842, 'backing': 2843, 'lacking': 2844, \"need's\": 2845, 'damn': 2846, 'whacking': 2847, 'forks': 2848, 'knives': 2849, 'bacon': 2850, 'goodlooking': 2851, 'dose': 2852, 'jackboots': 2853, 'kilt': 2854, 'killer': 2855, 'diller': 2856, 'hilt': 2857, 'attractively': 2858, 'heads': 2859, 'shi': 2860, 'nes': 2861, 'sip': 2862, 'lemonade': 2863, \"ev'rything's\": 2864, 'rains': 2865, 'schemes': 2866, 'forgotten': 2867, 'sailed': 2868, 'dawning': 2869, 'hero': 2870, 'handed': 2871, 'movie': 2872, 'stardom': 2873, 'fifth': 2874, 'november': 2875, 'junkman': 2876, 'sold': 2877, 'cheater': 2878, 'mistreater': 2879, 'evolution': 2880, 'destruction': 2881, 'solution': 2882, 'contribution': 2883, 'minds': 2884, 'constitution': 2885, 'institution': 2886, 'carrying': 2887, 'chairman': 2888, 'mao': 2889, 'anyhow': 2890, 'rip': 2891, 'hills': 2892, 'dakota': 2893, 'booked': 2894, 'saloon': 2895, 'equipped': 2896, 'stealing': 2897, 'magill': 2898, \"call'd\": 2899, 'lil': 2900, 'nancy': 2901, 'dan': 2902, 'hoedown': 2903, 'burst': 2904, 'grinning': 2905, \"'danny\": 2906, 'daniel': 2907, 'drew': 2908, 'collapsed': 2909, 'stinking': 2910, 'gin': 2911, 'proceeded': 2912, 'match': 2913, 'able': 2914, 'gideon': 2915, \"rocky's\": 2916, 'revival': 2917, 'rac': 2918, 'coon': 2919, 'toe': 2920, 'determined': 2921, 'aah': 2922, 'shoo': 2923, 'doo': 2924, \"p'doo\": 2925, \"tumblin'\": 2926, 'oooh': 2927, 'mmm': 2928, 'owned': 2929, 'lighten': 2930, 'raise': 2931, \"pepper'lonely\": 2932, 'certainly': 2933, 'audience': 2934, \"singer's\": 2935, 'billy': 2936, 'shears': 2937, 'protected': 2938, 'spoon': 2939, 'sucks': 2940, 'thumb': 2941, 'banks': 2942, 'lagoon': 2943, 'fifteen': 2944, 'clubs': 2945, 'department': 2946, 'rob': 2947, 'wednesday': 2948, 'silently': 2949, 'closing': 2950, 'bedroom': 2951, 'hoped': 2952, 'handkerchief': 2953, 'quietly': 2954, 'backdoor': 2955, 'steppping': 2956, 'sacrificed': 2957, 'snores': 2958, 'dressing': 2959, 'gown': 2960, 'stairs': 2961, 'cries': 2962, 'husband': 2963, 'thoughtlessly': 2964, 'struggled': 2965, 'appointment': 2966, 'trade': 2967, 'laways': 2968, 'tail': 2969, 'cheeks': 2970, 'rosie': 2971, 'nosey': 2972, \"clappin'\": 2973, \"poppin'\": 2974, 'sway': 2975, 'throw': 2976, 'battlefield': 2977, 'weapons': 2978, 'using': 2979, 'truest': 2980, 'squeezing': 2981, 'desire': 2982, \"sippin'\": 2983, 'moves': 2984, 'attracts': 2985, 'woos': 2986, 'misunderstanding': 2987, 'thee': 2988, 'quando': 2989, 'para': 2990, 'feliece': 2991, 'corazon': 2992, 'mundo': 2993, 'pararazzi': 2994, 'chicka': 2995, 'ferdy': 2996, 'parasol': 2997, 'cuesto': 2998, 'obrigado': 2999, 'tanta': 3000, 'carousel': 3001, 'crys': 3002, 'thirteen': 3003, 'marty': 3004, 'martyrs': 3005, 'derry': 3006, 'nailed': 3007, 'coffin': 3008, 'lidds': 3009, 'claim': 3010, 'majority': 3011, 'youâ€™re': 3012, 'minority': 3013, 'emerald': 3014, 'asle': 3015, 'stormont': 3016, 'bans': 3017, 'marchers': 3018, 'theyâ€™ve': 3019, 'internment': 3020, 'motherâ€™s': 3021, 'anglo': 3022, 'scotties': 3023, 'colonize': 3024, 'union': 3025, 'jacks': 3026, 'ransom': 3027, 'concentration': 3028, 'camps': 3029, 'falls': 3030, 'roads': 3031, 'repatriate': 3032, 'britain': 3033, 'rome': 3034, 'knocks': 3035, 'lands': 3036, 'shame': 3037, 'cools': 3038, 'claimed': 3039, 'framed': 3040, 'autopraphs': 3041, 'excited': 3042, 'grown': 3043, 'lipstick': 3044, 'sporting': 3045, 'heeled': 3046, 'trend': 3047, 'become': 3048, 'hurts': 3049, 'surround': 3050, 'nineteen': 3051, 'percent': 3052, 'thankful': 3053, 'seat': 3054, 'wilson': 3055, 'heath': 3056, 'advice': 3057, 'beware': 3058, 'pennies': 3059, 'tales': 3060, 'dad': 3061, 'ta': 3062, 'herself': 3063, 'partners': 3064, 'apply': 3065, 'suicide': 3066, 'turtle': 3067, 'doving': 3068, 'cupid': 3069, 'dart': 3070, 'boldly': 3071, 'bothered': 3072, 'ballad': 3073, 'dock': 3074, 'southampton': 3075, 'holland': 3076, 'france': 3077, 'mac': 3078, 'plane': 3079, 'mooning': 3080, 'seine': 3081, 'gibraltar': 3082, 'spain': 3083, 'amsterdam': 3084, 'hilton': 3085, 'beds': 3086, 'newspapers': 3087, \"what're\": 3088, 'rainy': 3089, 'charity': 3090, 'lighting': 3091, 'vienna': 3092, 'eating': 3093, 'choclate': 3094, 'gurus': 3095, 'acorns': 3096, 'tied': 3097, 'sack': 3098, 'press': 3099, 'success': 3100, 'continuing': 3101, 'hunting': 3102, 'elephant': 3103, 'accidents': 3104, 'mom': 3105, 'american': 3106, 'bullet': 3107, 'saxon': 3108, 'jungle': 3109, 'mighty': 3110, 'elephnats': 3111, 'captain': 3112, 'marvel': 3113, 'zapped': 3114, 'fierce': 3115, 'butted': 3116, 'equal': 3117, 'foolish': 3118, 'perfectly': 3119, 'hears': 3120, 'appears': 3121, 'listens': 3122, 'inner': 3123, 'wild': 3124, 'windy': 3125, 'washed': 3126, 'manytimes': 3127, 'manyways': 3128, 'shold': 3129, 'torture': 3130, 'hunger': 3131, 'beutty': 3132, 'raped': 3133, 'british': 3134, 'brigands': 3135, 'thereâ€™d': 3136, 'aharmock': 3137, 'morn': 3138, 'divided': 3139, 'glory': 3140, 'poets': 3141, 'auld': 3142, 'eireland': 3143, 'dew': 3144, 'galeway': 3145, 'leprechauns': 3146, 'beone': 3147, 'blarney': 3148, 'ira': 3149, 'commit': 3150, 'youâ€™s': 3151, 'woah': 3152}\n",
            "3153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6IuWOmyTen5"
      },
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# max_sequence_len = max([len(x) for x in input_sequences])\n",
        "max_sequence_len = 33\n",
        "# pad sequences\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "label = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2JGdKlSTi67",
        "outputId": "6453341c-3c52-4fbb-bbc0-2e48fa18e6c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 32, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 32, 32)            100896    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 128)               49664     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3153)              406737    \n",
            "=================================================================\n",
            "Total params: 557,297\n",
            "Trainable params: 557,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk7Kf4eh0qzc",
        "outputId": "f5ab5cd2-40be-4d75-819a-cff2313feec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "history = model.fit(predictors, label, epochs=20, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1290/1290 [==============================] - 12s 9ms/step - loss: 6.1132 - accuracy: 0.0479\n",
            "Epoch 2/20\n",
            "1290/1290 [==============================] - 12s 9ms/step - loss: 5.6542 - accuracy: 0.0642\n",
            "Epoch 3/20\n",
            "1290/1290 [==============================] - 12s 9ms/step - loss: 5.2818 - accuracy: 0.0945\n",
            "Epoch 4/20\n",
            "1290/1290 [==============================] - 12s 9ms/step - loss: 4.9630 - accuracy: 0.1246\n",
            "Epoch 5/20\n",
            "1290/1290 [==============================] - 11s 9ms/step - loss: 4.6903 - accuracy: 0.1540\n",
            "Epoch 6/20\n",
            "1290/1290 [==============================] - 12s 9ms/step - loss: 4.4470 - accuracy: 0.1782\n",
            "Epoch 7/20\n",
            "1290/1290 [==============================] - 12s 9ms/step - loss: 4.2131 - accuracy: 0.2045\n",
            "Epoch 8/20\n",
            "1290/1290 [==============================] - 11s 9ms/step - loss: 3.9970 - accuracy: 0.2321\n",
            "Epoch 9/20\n",
            "1290/1290 [==============================] - 11s 9ms/step - loss: 3.7910 - accuracy: 0.2601\n",
            "Epoch 10/20\n",
            "1290/1290 [==============================] - 11s 9ms/step - loss: 3.6031 - accuracy: 0.2845\n",
            "Epoch 11/20\n",
            "1290/1290 [==============================] - 11s 9ms/step - loss: 3.4362 - accuracy: 0.3063\n",
            "Epoch 12/20\n",
            "1290/1290 [==============================] - 11s 9ms/step - loss: 3.2819 - accuracy: 0.3290\n",
            "Epoch 13/20\n",
            "1290/1290 [==============================] - 11s 9ms/step - loss: 3.1427 - accuracy: 0.3503\n",
            "Epoch 14/20\n",
            "1290/1290 [==============================] - 11s 9ms/step - loss: 3.0174 - accuracy: 0.3697\n",
            "Epoch 15/20\n",
            "1290/1290 [==============================] - 11s 9ms/step - loss: 2.9037 - accuracy: 0.3881\n",
            "Epoch 16/20\n",
            "1290/1290 [==============================] - 11s 9ms/step - loss: 2.8063 - accuracy: 0.4043\n",
            "Epoch 17/20\n",
            "1290/1290 [==============================] - 11s 8ms/step - loss: 2.7130 - accuracy: 0.4203\n",
            "Epoch 18/20\n",
            "1290/1290 [==============================] - 11s 8ms/step - loss: 2.6228 - accuracy: 0.4375\n",
            "Epoch 19/20\n",
            "1290/1290 [==============================] - 11s 8ms/step - loss: 2.5445 - accuracy: 0.4493\n",
            "Epoch 20/20\n",
            "1290/1290 [==============================] - 11s 8ms/step - loss: 2.4729 - accuracy: 0.4637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0usM9leDWYj0",
        "outputId": "96312087-3a83-487d-fd78-8bacbcf4c605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLac19VEYJ75",
        "outputId": "247be944-381f-409d-bbe2-de8b676a872f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "seed_text = \"One day\"\n",
        "next_words = 10\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-d777fa149697>:7: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "One day the wind will blow it and you know that you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDTcPCytkDSu"
      },
      "source": [
        "## **4. Shakespeare Generator (Character Tokenization)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM8JxBcXwYqI"
      },
      "source": [
        "This tutorial demonstrates how to generate text using a character-based RNN. We will work with a dataset of Shakespeare's writing from Andrej Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks. Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdF6qytakFap",
        "outputId": "61882dc0-e1c7-47a3-ef9a-7d8de4954308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "#Download the dataset\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n",
        "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "#Explore the data\n",
        "text = open(path_to_file, 'r').read()\n",
        "print(text[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HhPI0QylYuf",
        "outputId": "837ddf55-9baa-4532-a72f-619629946e9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq35moPIntyT",
        "outputId": "e4807649-579c-4141-976f-6d9acc9722ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "for char,_ in zip(char2idx, range(5)):\n",
        "    print(repr(char), ':', char2idx[char])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'\\n' : 0\n",
            "' ' : 1\n",
            "'!' : 2\n",
            "'$' : 3\n",
            "'&' : 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qDhZElEn2RZ",
        "outputId": "4847e1cd-742b-42e5-815d-e9d1d597615b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBIS-PGjn4YA",
        "outputId": "00d1e2bd-fe7a-46db-8195-42224b058dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGsj19yun_aM"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asKeTJK1oEKr",
        "outputId": "78edf229-841a-469f-edeb-7ba6d3b75cf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcbsnOlBoFHp"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tcFbOj2oHFG"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkP4z3dYoKl3"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmvRLfXNoNL5",
        "outputId": "4195ec6e-0b9a-4913-ad8d-a692569109d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0mnaaNpoT2D"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzdETx6GoX9K"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOH937ZspOS0",
        "outputId": "708c7336-e46b-447e-f2db-5f003d29d83e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "import os\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "history = model.fit(dataset, epochs=5, callbacks=[checkpoint_callback], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "172/172 [==============================] - 13s 75ms/step - loss: 2.5718 - accuracy: 0.2932\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 13s 76ms/step - loss: 1.8794 - accuracy: 0.4495\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 13s 76ms/step - loss: 1.6341 - accuracy: 0.5154\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 1.5018 - accuracy: 0.5507\n",
            "Epoch 5/5\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 1.4216 - accuracy: 0.5710\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iFdSFjIviPk"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbfotJT-pi5_"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HygvXOurq2UU",
        "outputId": "c9a19efb-c539-4f43-9ccd-5bf86ff7a2d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "print(generate_text(model, start_string=\"Lord: \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lord: what seturn\n",
            "To the on in law, that was with you.\n",
            "\n",
            "Nurse:\n",
            "Let's that I'll wast too got.\n",
            "\n",
            "BRUTUS:\n",
            "What, more am yours;\n",
            "Fray not to do.\n",
            "\n",
            "EORWER:\n",
            "It is their fire exposed as mound my sons,\n",
            "And here be your hims, look lords and the fire of 3.\n",
            "\n",
            "KING RICHARD III:\n",
            "What to sitk a life, that, trust is distortuce.\n",
            "\n",
            "First Citizen:\n",
            "I must not, see mire, you mid confure to maid;\n",
            "Ay, his false be-not being needy relight\n",
            "Not when arr soul rubwing should by agging.\n",
            "New well aid to purpated in your breasty.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Bear him a rather, cry?\n",
            "\n",
            "Nurse:\n",
            "Lady you'll kissing a face of the deecherous cloud:\n",
            "When: are not risp truth to be married to shine be stays,\n",
            "Or do king, if all those gated to his:\n",
            "Away!\n",
            "\n",
            "PRONCEY:\n",
            "Is think not for my passy. If he come\n",
            "To call him my life: I pray, sin, if we may beed;\n",
            "And for winess till her weetifaticnes.\n",
            "Or she is a morrowning too majier,\n",
            "I resed the glit war to doober with theem: that he litesgep shel;\n",
            "I thank first seeate be happy take:\n",
            "We mus revented the beander\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKQnHcjUKZmI"
      },
      "source": [
        "## **(Optional) Text Generation using GPT-2 Transformer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwskL9bod5hB"
      },
      "source": [
        "OpenAI GPT-2 model was proposed in Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever. Itâ€™s a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n",
        "\n",
        "GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n",
        "\n",
        "Here, we will use a lighter version, [GPT-2 Medium Model](https://huggingface.co/gpt2-medium) for your convenience and demo purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CRWFJGmKlnP",
        "outputId": "a9ebb871-2922-41ef-d9d4-2a018f207b43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/fc/18e56e5b1093052bacf6750442410423f3d9785d14ce4f54ab2ac6b112a6/transformers-3.3.0-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 2.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 16.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 26.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=9e1e7ded16999ddde81b7a889f1f78056cdc54edb46e4d484051ce3fefa1f9a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIxzduTbU3DF"
      },
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, GPT2Config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTK-DbTnU0PH",
        "outputId": "f980f019-33e2-4298-bc6d-576c5a04fdfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290,
          "referenced_widgets": [
            "dc0d72800356409b8e2de18f81c8d16e",
            "218fa83966df4b18af04733c59988a64",
            "3656f295d235475f8b1e015118008d16",
            "199308992ba64bf2a3f79f1b91e89c84",
            "a0e0f75f7ae64d04b54a30b912400917",
            "1c58665b6f2c4ad5b62235ad67730e63",
            "9dc34912ae724498a932ab60cc11bf69",
            "373e0fa13978436abf42f9973f1483f8",
            "8ecc9b47987a46288db8552eb84e7397",
            "c359df3958e842eba36476dfd8812476",
            "cdd9601157f243f296ebb6d6ed83a52d",
            "ed3d8b2af7ff47139adb58fe152d7db0",
            "5b8bf364255f4687b0a74dd588159cb8",
            "8f26a7f48ac04790a3c0ad9b457d3528",
            "4f103cd0b693498385e45c6514438a3d",
            "a00c59a908554ce6b036e35fef2e4abb",
            "1cf1a8cb79c34946a7483064dbd87247",
            "30353a0e69964043ac8c731a75e93d59",
            "f87b20c79eef4949a2a85cbbc0266d68",
            "04e7d414d18b4d3480fcd15eb6728cc9",
            "41bb693986004c9ea868e99b44b58038",
            "c64730a85a294c829d923a8b7162d015",
            "9a65cc12c0044491b7524b69d7ca733c",
            "8a4831c328974ea092ae95a6fa5ab0a7",
            "e5d1fe83938f4401a8772760274c763a",
            "76f6ee379f2b43c39ee05b8164429928",
            "1a1cdc2ec173474da19fe8b29486d150",
            "b24abb978bb2433fae220fd9fea75982",
            "6d248996a5ed4241be55ba98f021c579",
            "6efd28e3afd84a7d82cf5e34f53d326f",
            "956dcee020d149cfb86dbedba50ceb02",
            "81f3cb7cb8c0470eb681a9c1f4304173"
          ]
        }
      },
      "source": [
        "model_name = \"gpt2-medium\"\n",
        "config = GPT2Config.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "gptmodel = TFGPT2LMHeadModel.from_pretrained(model_name, config=config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc0d72800356409b8e2de18f81c8d16e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=718.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ecc9b47987a46288db8552eb84e7397",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cf1a8cb79c34946a7483064dbd87247",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5d1fe83938f4401a8772760274c763a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1419628976.0, style=ProgressStyle(descrâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lt17WZYZ8-o"
      },
      "source": [
        "from transformers import set_seed\n",
        "set_seed(12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZA1oVCJVria",
        "outputId": "d0130557-5396-417a-b243-c8426c8df718",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "input_ids = tokenizer.encode('I love machine learning and data analytics,', return_tensors='tf')\n",
        "input_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 8), dtype=int32, numpy=\n",
              "array([[   40,  1842,  4572,  4673,   290,  1366, 23696,    11]],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdyttlcvWW9p",
        "outputId": "c0c5ff10-189a-4c2d-c107-8e29090dbbd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "output = gptmodel.generate(input_ids, max_length=15)\n",
        "print('Output:\\n')\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "\n",
            "I love machine learning and data analytics, but I'm not a big fan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua1NXKQiWpEp",
        "outputId": "894dc4cc-bc10-4943-ce70-bbdf1e75f0b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "sample_outputs = gptmodel.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=30,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=8\n",
        ")\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  \n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0: I love machine learning and data analytics, so I was excited to learn about R. My first thought was that R would be easier to learn than SQL\n",
            "1: I love machine learning and data analytics, but it's also really hard to put those skills together in a single class,\" says Kipnis, \"\n",
            "2: I love machine learning and data analytics, but most of all this machine learning is the most exciting and helpful for humans to understand. In other words,\n",
            "3: I love machine learning and data analytics, but I'm going to keep it to myself for now. Now, lets discuss some of the key aspects that\n",
            "4: I love machine learning and data analytics, but the software I use is not something you use to build a big machine like Google. So I'm not\n",
            "5: I love machine learning and data analytics, but I don't care how it works. I am concerned about making data intelligently representable. Here's\n",
            "6: I love machine learning and data analytics, especially the ones based on machine learning. And I love building things myself using Javascript, Python and the Google Comp\n",
            "7: I love machine learning and data analytics, but a lot of the new techniques we're using are based on the idea of data as information.\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}